{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 idxr packages the full lifecycle of an operational vector index into a repeatable playbook. The toolkit is model-centric \u2014you describe data with Pydantic models, config-driven \u2014you check configs into source control, and fail-stop-retry \u2014every stage records checkpoints so restarts are safe. Lifecycle at a Glance \u00b6 Create the First Index Use your model registry to scaffold preprocessing and vectorization configs. Generate partitions, hydrate an index, and lock the manifest snapshot in Git. Feed the Pipeline Daily As new data lands, rerun idxr prepare_datasets against the same config. Digest caches and row fingerprints keep subsequent runs fast while appending only novel partitions. Add New Models with Confidence New business areas slot in by registering fresh Pydantic models. Regenerate configs for those models only, review the diffs, and let idxr blend them into the same manifest. Evolve Schemas Safely Schema changes version themselves. When fields are added or renamed, idxr marks older partitions as stale and replays only the affected slices on the next vectorization run. Operate with Guardrails Every long-running job writes resumable state, detailed logs, and YAML error payloads for failed batches. You fix the source issue, rerun with --resume , and idxr carries on from the last good batch. Components \u00b6 idxr prepare_datasets \u2013 partitions raw CSVs/JSONL, normalises encodings, and keeps a manifest of partition \"migrations\". idxr vectorize \u2013 streams partitions into Chroma (local or cloud), enforces token budgets, and supports multi-partition concurrency. Query client \u2013 async Python client for multi-collection indexes with automatic fan-out, result merging, and model-based filtering. Shared libraries \u2013 offer model registries, manifest utilities, truncation strategies, drop orchestration, and logging helpers. Querying Multi-Collection Indexes \u00b6 When indexing large datasets (16M+ records), idxr distributes data across multiple ChromaDB collections. The query component provides seamless querying across these partitions: Generate query config \u2013 scan indexed partitions to map models to collections Initialize async client \u2013 connect to ChromaDB with the generated config Query with model filters \u2013 search specific models or all collections with automatic fan-out Retrieve merged results \u2013 get ranked results from multiple collections as if querying one See the Query documentation for complete details on querying multi-collection indexes. This documentation dives into each phase, explains every configuration surface, and links practical command-line recipes for daily operations. Continuous delivery of your knowledge base starts here.","title":"Overview"},{"location":"#overview","text":"idxr packages the full lifecycle of an operational vector index into a repeatable playbook. The toolkit is model-centric \u2014you describe data with Pydantic models, config-driven \u2014you check configs into source control, and fail-stop-retry \u2014every stage records checkpoints so restarts are safe.","title":"Overview"},{"location":"#lifecycle-at-a-glance","text":"Create the First Index Use your model registry to scaffold preprocessing and vectorization configs. Generate partitions, hydrate an index, and lock the manifest snapshot in Git. Feed the Pipeline Daily As new data lands, rerun idxr prepare_datasets against the same config. Digest caches and row fingerprints keep subsequent runs fast while appending only novel partitions. Add New Models with Confidence New business areas slot in by registering fresh Pydantic models. Regenerate configs for those models only, review the diffs, and let idxr blend them into the same manifest. Evolve Schemas Safely Schema changes version themselves. When fields are added or renamed, idxr marks older partitions as stale and replays only the affected slices on the next vectorization run. Operate with Guardrails Every long-running job writes resumable state, detailed logs, and YAML error payloads for failed batches. You fix the source issue, rerun with --resume , and idxr carries on from the last good batch.","title":"Lifecycle at a Glance"},{"location":"#components","text":"idxr prepare_datasets \u2013 partitions raw CSVs/JSONL, normalises encodings, and keeps a manifest of partition \"migrations\". idxr vectorize \u2013 streams partitions into Chroma (local or cloud), enforces token budgets, and supports multi-partition concurrency. Query client \u2013 async Python client for multi-collection indexes with automatic fan-out, result merging, and model-based filtering. Shared libraries \u2013 offer model registries, manifest utilities, truncation strategies, drop orchestration, and logging helpers.","title":"Components"},{"location":"#querying-multi-collection-indexes","text":"When indexing large datasets (16M+ records), idxr distributes data across multiple ChromaDB collections. The query component provides seamless querying across these partitions: Generate query config \u2013 scan indexed partitions to map models to collections Initialize async client \u2013 connect to ChromaDB with the generated config Query with model filters \u2013 search specific models or all collections with automatic fan-out Retrieve merged results \u2013 get ranked results from multiple collections as if querying one See the Query documentation for complete details on querying multi-collection indexes. This documentation dives into each phase, explains every configuration surface, and links practical command-line recipes for daily operations. Continuous delivery of your knowledge base starts here.","title":"Querying Multi-Collection Indexes"},{"location":"getting-started/","text":"Getting Started \u00b6 This guide walks you from a clean environment to a working idxr pipeline in minutes. Installation \u00b6 python -m venv .venv source .venv/bin/activate pip install --upgrade pip pip install idxr Optional helpers \u00b6 Install mkdocs if you plan to build the documentation locally: pip install mkdocs Quickstart Example \u00b6 The public idxr repository ships a miniature dataset that exercises the full lifecycle\u2014manifest generation, partitioning, and vectorization into an in-memory Chroma collection. # Clone the repository (or download the tarball) and jump into the example git clone https://github.com/GetAdriAI/idxr.git cd idxr/examples/quickstart # Create an isolated environment for the demo python -m venv .venv source .venv/bin/activate pip install --upgrade pip pip install -r requirements.txt # Provide credentials for embeddings (required by default strategies) export OPENAI_API_KEY = \"sk-your-key\" # 1. Prepare dataset partitions idxr prepare_datasets \\ --model quickstart.registry:MODEL_REGISTRY \\ --config config/prepare_datasets_config.json \\ --output-root workdir/partitions # 2. Index the generated partitions into a local Chroma store idxr vectorize index \\ --model quickstart.registry:MODEL_REGISTRY \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --collection quickstart \\ --batch-size 50 \\ --resume # 3. Inspect indexing status idxr vectorize status \\ --model quickstart.registry:MODEL_REGISTRY \\ --partition-dir workdir/partitions \\ --partition-out-dir workdir/chroma_partitions Each command is self-contained\u2014configs live under examples/quickstart/config , the demo registry is in quickstart/registry.py , and the sample CSV resides in data/contracts.csv . Change file paths or models to plug in your own domain once you are comfortable with the workflow. Querying Your Index \u00b6 Once indexing completes, query your multi-collection index with the async query client: # 1. Generate query configuration idxr vectorize generate-query-config \\ --partition-out-dir workdir/chroma_partitions \\ --output query_config.json \\ --model quickstart.registry:MODEL_REGISTRY # 2. Use in Python python -c \" from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import asyncio import os async def search(): async with AsyncMultiCollectionQueryClient( config_path=Path('query_config.json'), client_type='http', # or 'cloud' for ChromaDB Cloud http_host='localhost:8000', ) as client: results = await client.query( query_texts=['search term'], n_results=5, models=None, # Query all models ) for doc_id, distance in zip(results['ids'][0], results['distances'][0]): print(f'{doc_id}: {distance:.4f}') asyncio.run(search()) \" Next Steps \u00b6 Read the Prepare Datasets overview to understand how manifests are stitched together. Explore the Vectorize overview to learn how idxr batches and truncates content. Learn about Querying to search your multi-collection index efficiently. Consult the argument reference pages whenever you introduce new flags into your automation.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This guide walks you from a clean environment to a working idxr pipeline in minutes.","title":"Getting Started"},{"location":"getting-started/#installation","text":"python -m venv .venv source .venv/bin/activate pip install --upgrade pip pip install idxr","title":"Installation"},{"location":"getting-started/#optional-helpers","text":"Install mkdocs if you plan to build the documentation locally: pip install mkdocs","title":"Optional helpers"},{"location":"getting-started/#quickstart-example","text":"The public idxr repository ships a miniature dataset that exercises the full lifecycle\u2014manifest generation, partitioning, and vectorization into an in-memory Chroma collection. # Clone the repository (or download the tarball) and jump into the example git clone https://github.com/GetAdriAI/idxr.git cd idxr/examples/quickstart # Create an isolated environment for the demo python -m venv .venv source .venv/bin/activate pip install --upgrade pip pip install -r requirements.txt # Provide credentials for embeddings (required by default strategies) export OPENAI_API_KEY = \"sk-your-key\" # 1. Prepare dataset partitions idxr prepare_datasets \\ --model quickstart.registry:MODEL_REGISTRY \\ --config config/prepare_datasets_config.json \\ --output-root workdir/partitions # 2. Index the generated partitions into a local Chroma store idxr vectorize index \\ --model quickstart.registry:MODEL_REGISTRY \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --collection quickstart \\ --batch-size 50 \\ --resume # 3. Inspect indexing status idxr vectorize status \\ --model quickstart.registry:MODEL_REGISTRY \\ --partition-dir workdir/partitions \\ --partition-out-dir workdir/chroma_partitions Each command is self-contained\u2014configs live under examples/quickstart/config , the demo registry is in quickstart/registry.py , and the sample CSV resides in data/contracts.csv . Change file paths or models to plug in your own domain once you are comfortable with the workflow.","title":"Quickstart Example"},{"location":"getting-started/#querying-your-index","text":"Once indexing completes, query your multi-collection index with the async query client: # 1. Generate query configuration idxr vectorize generate-query-config \\ --partition-out-dir workdir/chroma_partitions \\ --output query_config.json \\ --model quickstart.registry:MODEL_REGISTRY # 2. Use in Python python -c \" from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import asyncio import os async def search(): async with AsyncMultiCollectionQueryClient( config_path=Path('query_config.json'), client_type='http', # or 'cloud' for ChromaDB Cloud http_host='localhost:8000', ) as client: results = await client.query( query_texts=['search term'], n_results=5, models=None, # Query all models ) for doc_id, distance in zip(results['ids'][0], results['distances'][0]): print(f'{doc_id}: {distance:.4f}') asyncio.run(search()) \"","title":"Querying Your Index"},{"location":"getting-started/#next-steps","text":"Read the Prepare Datasets overview to understand how manifests are stitched together. Explore the Vectorize overview to learn how idxr batches and truncates content. Learn about Querying to search your multi-collection index efficiently. Consult the argument reference pages whenever you introduce new flags into your automation.","title":"Next Steps"},{"location":"publishing/","text":"Publishing to GitHub Pages \u00b6 Use MkDocs to serve and publish these docs. The public repository already contains mkdocs.yml and the docs/ directory, so GitHub Pages can build the site automatically. 1. Install MkDocs and the Material theme (optional) \u00b6 pip install mkdocs mkdocs-material Material is optional\u2014the default theme works out of the box\u2014but it provides a polished navigation chrome. 2. Preview locally \u00b6 cd indexer mkdocs serve Navigate to http://127.0.0.1:8000/ to browse the site. MkDocs reloads pages whenever you edit Markdown files. 3. Publish with GitHub Actions (recommended) \u00b6 In the public repo at GetAdriAI/idxr , enable GitHub Pages with the GitHub Actions source. Add a workflow (for example .github/workflows/docs.yml ): name : Build docs on : push : branches : [ main ] workflow_dispatch : jobs : deploy : permissions : contents : write runs-on : ubuntu-latest steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : '3.12' - run : pip install mkdocs mkdocs-material - run : mkdocs gh-deploy --force Merge to main . The workflow pushes the rendered site to the gh-pages branch that GitHub Pages serves. 4. Manual publish (fallback) \u00b6 If you prefer not to use Actions, run: mkdocs gh-deploy --force This command builds the static site, pushes it to the gh-pages branch of the active remote, and prints the Pages URL. 5. Keeping docs in sync \u00b6 Run mkdocs build --strict in CI to catch broken links before merging. Update the docs whenever you add or rename CLI flags. The navigation already groups pages by command and argument. After publishing a new package release, sync the public repo (using scripts/sync_public_repo.sh --tag ) so the documentation reflects the same version that shipped to PyPI.","title":"Publishing to GitHub Pages"},{"location":"publishing/#publishing-to-github-pages","text":"Use MkDocs to serve and publish these docs. The public repository already contains mkdocs.yml and the docs/ directory, so GitHub Pages can build the site automatically.","title":"Publishing to GitHub Pages"},{"location":"publishing/#1-install-mkdocs-and-the-material-theme-optional","text":"pip install mkdocs mkdocs-material Material is optional\u2014the default theme works out of the box\u2014but it provides a polished navigation chrome.","title":"1. Install MkDocs and the Material theme (optional)"},{"location":"publishing/#2-preview-locally","text":"cd indexer mkdocs serve Navigate to http://127.0.0.1:8000/ to browse the site. MkDocs reloads pages whenever you edit Markdown files.","title":"2. Preview locally"},{"location":"publishing/#3-publish-with-github-actions-recommended","text":"In the public repo at GetAdriAI/idxr , enable GitHub Pages with the GitHub Actions source. Add a workflow (for example .github/workflows/docs.yml ): name : Build docs on : push : branches : [ main ] workflow_dispatch : jobs : deploy : permissions : contents : write runs-on : ubuntu-latest steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : '3.12' - run : pip install mkdocs mkdocs-material - run : mkdocs gh-deploy --force Merge to main . The workflow pushes the rendered site to the gh-pages branch that GitHub Pages serves.","title":"3. Publish with GitHub Actions (recommended)"},{"location":"publishing/#4-manual-publish-fallback","text":"If you prefer not to use Actions, run: mkdocs gh-deploy --force This command builds the static site, pushes it to the gh-pages branch of the active remote, and prints the Pages URL.","title":"4. Manual publish (fallback)"},{"location":"publishing/#5-keeping-docs-in-sync","text":"Run mkdocs build --strict in CI to catch broken links before merging. Update the docs whenever you add or rename CLI flags. The navigation already groups pages by command and argument. After publishing a new package release, sync the public repo (using scripts/sync_public_repo.sh --tag ) so the documentation reflects the same version that shipped to PyPI.","title":"5. Keeping docs in sync"},{"location":"prepare-datasets/config/","text":"Prepare Dataset Config Reference \u00b6 idxr prepare_datasets consumes a JSON document that maps each Pydantic model name to a preprocessing recipe. A minimal config looks like: { \"Contract\" : { \"path\" : \"datasets/contracts.csv\" , \"columns\" : { \"id\" : \"CONTRACT_ID\" , \"title\" : \"CONTRACT_TITLE\" , \"summary\" : \"DESCRIPTION\" }, \"character_encoding\" : \"utf-8\" , \"delimiter\" : \",\" , \"malformed_column\" : null , \"header_row\" : \"all\" , \"drop_na_columns\" : [] } } Each field plays a specific role during preprocessing: Field Required Description path \u2705 Absolute or relative path to the source CSV/JSONL file. Leave blank ( \"\" ) to skip a model temporarily. columns \u2705 Mapping of model field name \u2192 source column header. Add, remove, or rename entries to match the schema expected by the Pydantic model. character_encoding optional (defaults to \"utf-8\" ) Target encoding for the dataset. idxr decodes using this value and re-encodes output partitions as UTF-8. delimiter optional (defaults to \",\" ) Column delimiter for CSV inputs. Change to \"\\t\" for TSV or \";\" if your exports use semicolons. malformed_column optional Zero-based index of a column that frequently contains embedded newlines. idxr stitches rows by looking ahead until it can parse this column. header_row optional (defaults to \"all\" ) Controls which rows are considered headers: \"all\" keeps every header row, \"first\" retains only the first row, or specify a literal string to match. drop_na_columns optional (defaults to [] ) List of column names that must be non-empty. Rows with empty/ NaN values in these columns are dropped before partitioning. Tips \u00b6 Store configs under version control. They serve as the contract between data engineering and knowledge engineering teams. When CSV exports move, update just the path \u2014manifest diffing prevents reprocessing unchanged rows. Use separate configs for different sourcing strategies (e.g., nightly full export vs. targeted hotfix).","title":"Config Reference"},{"location":"prepare-datasets/config/#prepare-dataset-config-reference","text":"idxr prepare_datasets consumes a JSON document that maps each Pydantic model name to a preprocessing recipe. A minimal config looks like: { \"Contract\" : { \"path\" : \"datasets/contracts.csv\" , \"columns\" : { \"id\" : \"CONTRACT_ID\" , \"title\" : \"CONTRACT_TITLE\" , \"summary\" : \"DESCRIPTION\" }, \"character_encoding\" : \"utf-8\" , \"delimiter\" : \",\" , \"malformed_column\" : null , \"header_row\" : \"all\" , \"drop_na_columns\" : [] } } Each field plays a specific role during preprocessing: Field Required Description path \u2705 Absolute or relative path to the source CSV/JSONL file. Leave blank ( \"\" ) to skip a model temporarily. columns \u2705 Mapping of model field name \u2192 source column header. Add, remove, or rename entries to match the schema expected by the Pydantic model. character_encoding optional (defaults to \"utf-8\" ) Target encoding for the dataset. idxr decodes using this value and re-encodes output partitions as UTF-8. delimiter optional (defaults to \",\" ) Column delimiter for CSV inputs. Change to \"\\t\" for TSV or \";\" if your exports use semicolons. malformed_column optional Zero-based index of a column that frequently contains embedded newlines. idxr stitches rows by looking ahead until it can parse this column. header_row optional (defaults to \"all\" ) Controls which rows are considered headers: \"all\" keeps every header row, \"first\" retains only the first row, or specify a literal string to match. drop_na_columns optional (defaults to [] ) List of column names that must be non-empty. Rows with empty/ NaN values in these columns are dropped before partitioning.","title":"Prepare Dataset Config Reference"},{"location":"prepare-datasets/config/#tips","text":"Store configs under version control. They serve as the contract between data engineering and knowledge engineering teams. When CSV exports move, update just the path \u2014manifest diffing prevents reprocessing unchanged rows. Use separate configs for different sourcing strategies (e.g., nightly full export vs. targeted hotfix).","title":"Tips"},{"location":"prepare-datasets/overview/","text":"Prepare Datasets \u00b6 idxr prepare_datasets turns raw CSV/JSONL exports into manifest-tracked partitions. It is the \u201cmigration authoring\u201d phase of the lifecycle\u2014you build reproducible input slices that downstream vectorization can replay in order. Responsibilities \u00b6 Schema awareness \u2013 uses your Pydantic model registry to validate column mappings and detect schema drift. Row hygiene \u2013 trims whitespace, fixes malformed encodings, stitches newline-leaking rows, and drops duplicates via deterministic digests. Partition orchestration \u2013 writes partitions into timestamped directories while maintaining a single manifest that records model, partition, and schema version metadata. Change tracking \u2013 records row-level digests alongside the manifest so reruns skip previously processed records. Drop planning \u2013 generates and applies remediation scripts that mark partitions as stale or deleted when you want to unwind a bad migration. Workflow Summary \u00b6 Scaffold a config with idxr prepare_datasets new-config . The config lists every model and generates a column mapping stub. Edit the config to point at the CSV exports you actually want to ingest. Run idxr prepare_datasets with the config and an output directory. Repeated runs append new partitions if the source data changed. Review the manifest ( manifest.json ) to audit what was produced, including digests and schema signatures. Plan drops with idxr prepare_datasets plan-drop and execute them with idxr prepare_datasets apply-drop whenever you need to roll back a migration. The rest of this section documents the configuration schema and command-line surface area so you can tailor the pipeline to your own datasets.","title":"Purpose & Workflow"},{"location":"prepare-datasets/overview/#prepare-datasets","text":"idxr prepare_datasets turns raw CSV/JSONL exports into manifest-tracked partitions. It is the \u201cmigration authoring\u201d phase of the lifecycle\u2014you build reproducible input slices that downstream vectorization can replay in order.","title":"Prepare Datasets"},{"location":"prepare-datasets/overview/#responsibilities","text":"Schema awareness \u2013 uses your Pydantic model registry to validate column mappings and detect schema drift. Row hygiene \u2013 trims whitespace, fixes malformed encodings, stitches newline-leaking rows, and drops duplicates via deterministic digests. Partition orchestration \u2013 writes partitions into timestamped directories while maintaining a single manifest that records model, partition, and schema version metadata. Change tracking \u2013 records row-level digests alongside the manifest so reruns skip previously processed records. Drop planning \u2013 generates and applies remediation scripts that mark partitions as stale or deleted when you want to unwind a bad migration.","title":"Responsibilities"},{"location":"prepare-datasets/overview/#workflow-summary","text":"Scaffold a config with idxr prepare_datasets new-config . The config lists every model and generates a column mapping stub. Edit the config to point at the CSV exports you actually want to ingest. Run idxr prepare_datasets with the config and an output directory. Repeated runs append new partitions if the source data changed. Review the manifest ( manifest.json ) to audit what was produced, including digests and schema signatures. Plan drops with idxr prepare_datasets plan-drop and execute them with idxr prepare_datasets apply-drop whenever you need to roll back a migration. The rest of this section documents the configuration schema and command-line surface area so you can tailor the pipeline to your own datasets.","title":"Workflow Summary"},{"location":"prepare-datasets/args/config/","text":"--config \u00b6 Why we added this flag: idxr needs a concrete mapping between models and source files; the config flag lets you point to that JSON document each time you run the pipeline. What it does \u00b6 Path to the prepare-datasets JSON config (see Config Reference ). Required so idxr knows which source files to read and how to map columns. Supports relative paths; they are resolved from the current working directory. Typical usage \u00b6 idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config workdir/configs/prepare_datasets_contracts.json \\ --output-root workdir/partitions Tips \u00b6 Keep configs under workdir/prepare_datasets (or similar) so they are easy to diff in Git. Use different configs for full exports vs. patch drops to keep manifests easy to audit.","title":"--config"},{"location":"prepare-datasets/args/config/#-config","text":"Why we added this flag: idxr needs a concrete mapping between models and source files; the config flag lets you point to that JSON document each time you run the pipeline.","title":"--config"},{"location":"prepare-datasets/args/config/#what-it-does","text":"Path to the prepare-datasets JSON config (see Config Reference ). Required so idxr knows which source files to read and how to map columns. Supports relative paths; they are resolved from the current working directory.","title":"What it does"},{"location":"prepare-datasets/args/config/#typical-usage","text":"idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config workdir/configs/prepare_datasets_contracts.json \\ --output-root workdir/partitions","title":"Typical usage"},{"location":"prepare-datasets/args/config/#tips","text":"Keep configs under workdir/prepare_datasets (or similar) so they are easy to diff in Git. Use different configs for full exports vs. patch drops to keep manifests easy to audit.","title":"Tips"},{"location":"prepare-datasets/args/directory-size/","text":"--directory-size \u00b6 Why we added this flag: large exports can overwhelm filesystem limits; directory sizing caps how many rows a partition may contain so runs stay manageable. What it does \u00b6 Sets the maximum number of rows per model per partition directory. 0 (default) means unlimited rows; any positive integer enforces a split. Useful when chunking massive tables for parallel indexing or archiving. Typical usage \u00b6 idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/full_export.json \\ --output-root workdir/partitions \\ --directory-size 500000 Tips \u00b6 Match the partition size to the throughput of your vectorization job\u2014smaller chunks resume faster after failures. Monitor manifest growth; each new directory is recorded with schema and timestamp metadata.","title":"--directory-size"},{"location":"prepare-datasets/args/directory-size/#-directory-size","text":"Why we added this flag: large exports can overwhelm filesystem limits; directory sizing caps how many rows a partition may contain so runs stay manageable.","title":"--directory-size"},{"location":"prepare-datasets/args/directory-size/#what-it-does","text":"Sets the maximum number of rows per model per partition directory. 0 (default) means unlimited rows; any positive integer enforces a split. Useful when chunking massive tables for parallel indexing or archiving.","title":"What it does"},{"location":"prepare-datasets/args/directory-size/#typical-usage","text":"idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/full_export.json \\ --output-root workdir/partitions \\ --directory-size 500000","title":"Typical usage"},{"location":"prepare-datasets/args/directory-size/#tips","text":"Match the partition size to the throughput of your vectorization job\u2014smaller chunks resume faster after failures. Monitor manifest growth; each new directory is recorded with schema and timestamp metadata.","title":"Tips"},{"location":"prepare-datasets/args/log-level/","text":"--log-level \u00b6 Why we added this flag: long-running jobs need adjustable verbosity for debugging; this switch lets you promote or suppress log noise without editing code. What it does \u00b6 Sets the root logging level ( DEBUG , INFO , WARNING , ERROR ). Defaults to INFO . Works with both console output and file handlers configured via environment variables. Typical usage \u00b6 idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/full_export.json \\ --output-root workdir/partitions \\ --log-level DEBUG Tips \u00b6 Use DEBUG when investigating malformed rows; the log stream includes row indices and remediation actions. Keep production runs at INFO or higher to avoid massive log files.","title":"--log-level"},{"location":"prepare-datasets/args/log-level/#-log-level","text":"Why we added this flag: long-running jobs need adjustable verbosity for debugging; this switch lets you promote or suppress log noise without editing code.","title":"--log-level"},{"location":"prepare-datasets/args/log-level/#what-it-does","text":"Sets the root logging level ( DEBUG , INFO , WARNING , ERROR ). Defaults to INFO . Works with both console output and file handlers configured via environment variables.","title":"What it does"},{"location":"prepare-datasets/args/log-level/#typical-usage","text":"idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/full_export.json \\ --output-root workdir/partitions \\ --log-level DEBUG","title":"Typical usage"},{"location":"prepare-datasets/args/log-level/#tips","text":"Use DEBUG when investigating malformed rows; the log stream includes row indices and remediation actions. Keep production runs at INFO or higher to avoid massive log files.","title":"Tips"},{"location":"prepare-datasets/args/manifest/","text":"--manifest \u00b6 Why we added this flag: reruns need continuity\u2014by pointing at an existing manifest, idxr layers new partitions on top of the previous history instead of overwriting it. What it does \u00b6 Allows you to reuse an existing manifest ( manifest.json ) rather than letting idxr create a fresh one under the output root. Handy when two configs contribute to the same manifest (e.g., a hotfix layered onto a full export). Optional; if omitted, idxr defaults to <output-root>/manifest.json . Typical usage \u00b6 idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/hotfix.json \\ --output-root workdir/partitions \\ --manifest workdir/partitions/manifest.json Tips \u00b6 Always back up the manifest before large migrations; it is the authoritative history of processed rows. When experimenting, target a separate manifest (e.g., under workdir/sandboxes ) to avoid contaminating production history.","title":"--manifest"},{"location":"prepare-datasets/args/manifest/#-manifest","text":"Why we added this flag: reruns need continuity\u2014by pointing at an existing manifest, idxr layers new partitions on top of the previous history instead of overwriting it.","title":"--manifest"},{"location":"prepare-datasets/args/manifest/#what-it-does","text":"Allows you to reuse an existing manifest ( manifest.json ) rather than letting idxr create a fresh one under the output root. Handy when two configs contribute to the same manifest (e.g., a hotfix layered onto a full export). Optional; if omitted, idxr defaults to <output-root>/manifest.json .","title":"What it does"},{"location":"prepare-datasets/args/manifest/#typical-usage","text":"idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/hotfix.json \\ --output-root workdir/partitions \\ --manifest workdir/partitions/manifest.json","title":"Typical usage"},{"location":"prepare-datasets/args/manifest/#tips","text":"Always back up the manifest before large migrations; it is the authoritative history of processed rows. When experimenting, target a separate manifest (e.g., under workdir/sandboxes ) to avoid contaminating production history.","title":"Tips"},{"location":"prepare-datasets/args/model/","text":"--model \u00b6 Why we added this flag: every dataset run needs to know which Pydantic model registry to validate against; making it explicit avoids accidentally targeting the wrong schema bundle. What it does \u00b6 Accepts a Python import string ( package.module:ATTRIBUTE ) that resolves to a mapping of model names to ModelSpec objects. Drives schema validation, field listing, and schema signature hashing for manifest tracking. Required for all idxr prepare_datasets invocations. Typical usage \u00b6 idxr prepare_datasets \\ --model kb.std.ecc_6_0_ehp_7.registry:MODEL_REGISTRY \\ --config configs/hotfix.json \\ --output-root build/partitions Tips \u00b6 Reference the same registry for prepare and vectorize so schema signatures stay aligned. Use environment variables if you switch registries frequently: export IDXR_MODEL=kb.std... and pass --model \"$IDXR_MODEL\" .","title":"--model"},{"location":"prepare-datasets/args/model/#-model","text":"Why we added this flag: every dataset run needs to know which Pydantic model registry to validate against; making it explicit avoids accidentally targeting the wrong schema bundle.","title":"--model"},{"location":"prepare-datasets/args/model/#what-it-does","text":"Accepts a Python import string ( package.module:ATTRIBUTE ) that resolves to a mapping of model names to ModelSpec objects. Drives schema validation, field listing, and schema signature hashing for manifest tracking. Required for all idxr prepare_datasets invocations.","title":"What it does"},{"location":"prepare-datasets/args/model/#typical-usage","text":"idxr prepare_datasets \\ --model kb.std.ecc_6_0_ehp_7.registry:MODEL_REGISTRY \\ --config configs/hotfix.json \\ --output-root build/partitions","title":"Typical usage"},{"location":"prepare-datasets/args/model/#tips","text":"Reference the same registry for prepare and vectorize so schema signatures stay aligned. Use environment variables if you switch registries frequently: export IDXR_MODEL=kb.std... and pass --model \"$IDXR_MODEL\" .","title":"Tips"},{"location":"prepare-datasets/args/output-root/","text":"--output-root \u00b6 Why we added this flag: partition outputs and manifests belong in a predictable workspace so they can be shared with vectorize; this flag names that root directory. What it does \u00b6 Specifies where idxr should create partition directories and the manifest.json . Required for every run; idxr creates the directory if it does not exist. Using the same output root across runs lets idxr append new partitions and reuse digests. Typical usage \u00b6 idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/hotfix.json \\ --output-root workdir/partitions Tips \u00b6 Commit the manifest file to source control to capture a history of migrations. Keep the output root outside of your source tree if partitions are large; symlink the manifest back into Git if needed.","title":"--output-root"},{"location":"prepare-datasets/args/output-root/#-output-root","text":"Why we added this flag: partition outputs and manifests belong in a predictable workspace so they can be shared with vectorize; this flag names that root directory.","title":"--output-root"},{"location":"prepare-datasets/args/output-root/#what-it-does","text":"Specifies where idxr should create partition directories and the manifest.json . Required for every run; idxr creates the directory if it does not exist. Using the same output root across runs lets idxr append new partitions and reuse digests.","title":"What it does"},{"location":"prepare-datasets/args/output-root/#typical-usage","text":"idxr prepare_datasets \\ --model \" $IDXR_MODEL \" \\ --config configs/hotfix.json \\ --output-root workdir/partitions","title":"Typical usage"},{"location":"prepare-datasets/args/output-root/#tips","text":"Commit the manifest file to source control to capture a history of migrations. Keep the output root outside of your source tree if partitions are large; symlink the manifest back into Git if needed.","title":"Tips"},{"location":"query/api-reference/","text":"API Reference \u00b6 Complete reference for the AsyncMultiCollectionQueryClient and related functions. AsyncMultiCollectionQueryClient \u00b6 Async client for querying multi-collection ChromaDB indexes with model-based filtering. Constructor \u00b6 AsyncMultiCollectionQueryClient ( config_path : Path , * , client_type : str = \"http\" , http_host : Optional [ str ] = None , http_port : Optional [ int ] = None , http_ssl : bool = False , http_headers : Optional [ Mapping [ str , str ]] = None , cloud_api_key : Optional [ str ] = None , cloud_tenant : Optional [ str ] = None , cloud_database : Optional [ str ] = None , embedding_function : Optional [ EmbeddingFunction ] = None , ) Parameters: config_path (Path): Path to query config JSON file generated by generate-query-config client_type (str): ChromaDB client type - \"http\" for self-hosted or \"cloud\" for ChromaDB Cloud http_host (str, optional): Hostname for HTTP client (e.g., \"localhost:8000\" ) http_port (int, optional): Port for HTTP client (default: 8000) http_ssl (bool): Whether to use SSL for HTTP client (default: False) http_headers (Mapping[str, str], optional): Custom headers for HTTP client cloud_api_key (str, optional): API key for ChromaDB Cloud cloud_tenant (str, optional): Tenant ID for ChromaDB Cloud cloud_database (str, optional): Database name for ChromaDB Cloud embedding_function (EmbeddingFunction, optional): Custom embedding function Usage: # HTTP client (self-hosted) client = AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"http\" , http_host = \"localhost:8000\" , ) # Cloud client client = AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) Methods \u00b6 query() \u00b6 Query across multiple collections with automatic fan-out and result merging. async def query ( * , query_embeddings : Optional [ Sequence [ Sequence [ float ]]] = None , query_texts : Optional [ Sequence [ str ]] = None , n_results : int = 10 , where : Optional [ Mapping [ str , Any ]] = None , where_document : Optional [ WhereDocument ] = None , include : Optional [ Include ] = None , models : Optional [ Sequence [ str ]] = None , ) -> QueryResult Parameters: query_embeddings (Sequence[Sequence[float]], optional): Query embeddings (provide either this or query_texts) query_texts (Sequence[str], optional): Query texts to embed (provide either this or query_embeddings) n_results (int): Maximum number of results to return (default: 10) where (Mapping[str, Any], optional): ChromaDB metadata filter where_document (WhereDocument, optional): ChromaDB document content filter include (Include, optional): Fields to include in results (default: [\"metadatas\", \"documents\", \"distances\"]) models (Sequence[str], optional): Model names to query. If None, queries all collections Returns: QueryResult : Dictionary containing: ids : List of document IDs per query distances : List of distance scores per query documents : List of document texts per query metadatas : List of metadata dicts per query embeddings : Optional embeddings if included Example: # Query with texts results = await client . query ( query_texts = [ \"SAP authorization\" ], n_results = 10 , models = [ \"Table\" , \"Field\" ], ) # Query with embeddings results = await client . query ( query_embeddings = [[ 0.1 , 0.2 , 0.3 , ... ]], n_results = 10 , models = None , ) # Query with filters results = await client . query ( query_texts = [ \"customer data\" ], n_results = 10 , where = { \"has_sem\" : True , \"schema_version\" : { \"$gte\" : 2 }}, models = [ \"Table\" ], ) get() \u00b6 Retrieve documents by ID or filter from multiple collections. async def get ( * , ids : Optional [ Sequence [ str ]] = None , where : Optional [ Mapping [ str , Any ]] = None , where_document : Optional [ WhereDocument ] = None , limit : Optional [ int ] = None , offset : Optional [ int ] = None , include : Optional [ Include ] = None , models : Optional [ Sequence [ str ]] = None , ) -> Dict [ str , Any ] Parameters: ids (Sequence[str], optional): Document IDs to retrieve where (Mapping[str, Any], optional): Metadata filter where_document (WhereDocument, optional): Document content filter limit (int, optional): Maximum number of documents to return offset (int, optional): Number of documents to skip include (Include, optional): Fields to include (default: [\"metadatas\", \"documents\"]) models (Sequence[str], optional): Model names to query Returns: Dictionary containing ids, documents, metadatas, etc. Example: # Get by IDs docs = await client . get ( ids = [ \"doc_123\" , \"doc_456\" ], models = [ \"Table\" ], ) # Get with filter docs = await client . get ( where = { \"has_sem\" : True }, limit = 100 , models = None , ) count() \u00b6 Count total documents across collections. async def count ( * , where : Optional [ Mapping [ str , Any ]] = None , models : Optional [ Sequence [ str ]] = None , ) -> int Parameters: where (Mapping[str, Any], optional): Metadata filter models (Sequence[str], optional): Model names to count Returns: Total document count across queried collections Example: # Count all documents total = await client . count ( models = None ) # Count specific model table_count = await client . count ( models = [ \"Table\" ]) # Count with filter sem_count = await client . count ( where = { \"has_sem\" : True }, models = None , ) connect() / close() \u00b6 Manually manage connection lifecycle (typically use context manager instead). async def connect () -> None async def close () -> None Example: # Manual lifecycle management client = AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" )) await client . connect () try : results = await client . query ( query_texts = [ \"search\" ], n_results = 10 ) finally : await client . close () # Recommended: use context manager async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" )) as client : results = await client . query ( query_texts = [ \"search\" ], n_results = 10 ) Query Config Functions \u00b6 generate_query_config() \u00b6 Generate query configuration from indexed partitions. def generate_query_config ( partition_out_dir : Path , * , output_path : Optional [ Path ] = None , collection_prefix : Optional [ str ] = None , ) -> Dict [ str , Any ] Parameters: partition_out_dir (Path): Directory containing partition subdirectories with resume state files output_path (Path, optional): Path to write query config JSON (if not provided, returns dict only) collection_prefix (str, optional): Optional prefix added to collection names Returns: Query configuration dictionary Example: from indexer.vectorize_lib import generate_query_config from pathlib import Path config = generate_query_config ( partition_out_dir = Path ( \"build/vector\" ), output_path = Path ( \"query_config.json\" ), collection_prefix = \"ecc-prod\" , ) print ( f \"Generated config with { config [ 'metadata' ][ 'total_models' ] } models\" ) load_query_config() \u00b6 Load and validate query configuration from JSON file. def load_query_config ( config_path : Path ) -> Dict [ str , Any ] Parameters: config_path (Path): Path to query config JSON file Returns: Query configuration dictionary Raises: ValueError : If file doesn't exist, is invalid JSON, or missing required keys Example: from indexer.vectorize_lib import load_query_config from pathlib import Path config = load_query_config ( Path ( \"query_config.json\" )) print ( f \"Loaded config with { len ( config [ 'model_to_collections' ]) } models\" ) get_collections_for_models() \u00b6 Get list of collections to query for given model names. def get_collections_for_models ( query_config : Dict [ str , Any ], model_names : Optional [ List [ str ]] = None , ) -> List [ str ] Parameters: query_config (Dict[str, Any]): Query configuration dictionary model_names (List[str], optional): Model names to query. If None, returns all collections Returns: List of collection names Example: from indexer.vectorize_lib import load_query_config , get_collections_for_models from pathlib import Path config = load_query_config ( Path ( \"query_config.json\" )) # Get collections for specific models collections = get_collections_for_models ( config , [ \"Table\" , \"Field\" ]) print ( f \"Collections: { collections } \" ) # [\"partition_00001\", \"partition_00002\", ...] # Get all collections all_collections = get_collections_for_models ( config , None ) print ( f \"All collections: { all_collections } \" ) ChromaDB Metadata Filters \u00b6 The where parameter supports ChromaDB's metadata filtering operators: # Equality where = { \"model_name\" : \"Table\" } # Comparison where = { \"schema_version\" : { \"$gte\" : 2 }} # In/Not In where = { \"model_name\" : { \"$in\" : [ \"Table\" , \"Field\" ]}} # Logical operators where = { \"$and\" : [ { \"has_sem\" : True }, { \"schema_version\" : { \"$gte\" : 2 }} ] } Supported operators: $eq : Equal $ne : Not equal $gt : Greater than $gte : Greater than or equal $lt : Less than $lte : Less than or equal $in : In list $nin : Not in list $and : Logical AND $or : Logical OR See ChromaDB metadata filtering documentation for more details.","title":"API Reference"},{"location":"query/api-reference/#api-reference","text":"Complete reference for the AsyncMultiCollectionQueryClient and related functions.","title":"API Reference"},{"location":"query/api-reference/#asyncmulticollectionqueryclient","text":"Async client for querying multi-collection ChromaDB indexes with model-based filtering.","title":"AsyncMultiCollectionQueryClient"},{"location":"query/api-reference/#constructor","text":"AsyncMultiCollectionQueryClient ( config_path : Path , * , client_type : str = \"http\" , http_host : Optional [ str ] = None , http_port : Optional [ int ] = None , http_ssl : bool = False , http_headers : Optional [ Mapping [ str , str ]] = None , cloud_api_key : Optional [ str ] = None , cloud_tenant : Optional [ str ] = None , cloud_database : Optional [ str ] = None , embedding_function : Optional [ EmbeddingFunction ] = None , ) Parameters: config_path (Path): Path to query config JSON file generated by generate-query-config client_type (str): ChromaDB client type - \"http\" for self-hosted or \"cloud\" for ChromaDB Cloud http_host (str, optional): Hostname for HTTP client (e.g., \"localhost:8000\" ) http_port (int, optional): Port for HTTP client (default: 8000) http_ssl (bool): Whether to use SSL for HTTP client (default: False) http_headers (Mapping[str, str], optional): Custom headers for HTTP client cloud_api_key (str, optional): API key for ChromaDB Cloud cloud_tenant (str, optional): Tenant ID for ChromaDB Cloud cloud_database (str, optional): Database name for ChromaDB Cloud embedding_function (EmbeddingFunction, optional): Custom embedding function Usage: # HTTP client (self-hosted) client = AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"http\" , http_host = \"localhost:8000\" , ) # Cloud client client = AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , )","title":"Constructor"},{"location":"query/api-reference/#methods","text":"","title":"Methods"},{"location":"query/api-reference/#query","text":"Query across multiple collections with automatic fan-out and result merging. async def query ( * , query_embeddings : Optional [ Sequence [ Sequence [ float ]]] = None , query_texts : Optional [ Sequence [ str ]] = None , n_results : int = 10 , where : Optional [ Mapping [ str , Any ]] = None , where_document : Optional [ WhereDocument ] = None , include : Optional [ Include ] = None , models : Optional [ Sequence [ str ]] = None , ) -> QueryResult Parameters: query_embeddings (Sequence[Sequence[float]], optional): Query embeddings (provide either this or query_texts) query_texts (Sequence[str], optional): Query texts to embed (provide either this or query_embeddings) n_results (int): Maximum number of results to return (default: 10) where (Mapping[str, Any], optional): ChromaDB metadata filter where_document (WhereDocument, optional): ChromaDB document content filter include (Include, optional): Fields to include in results (default: [\"metadatas\", \"documents\", \"distances\"]) models (Sequence[str], optional): Model names to query. If None, queries all collections Returns: QueryResult : Dictionary containing: ids : List of document IDs per query distances : List of distance scores per query documents : List of document texts per query metadatas : List of metadata dicts per query embeddings : Optional embeddings if included Example: # Query with texts results = await client . query ( query_texts = [ \"SAP authorization\" ], n_results = 10 , models = [ \"Table\" , \"Field\" ], ) # Query with embeddings results = await client . query ( query_embeddings = [[ 0.1 , 0.2 , 0.3 , ... ]], n_results = 10 , models = None , ) # Query with filters results = await client . query ( query_texts = [ \"customer data\" ], n_results = 10 , where = { \"has_sem\" : True , \"schema_version\" : { \"$gte\" : 2 }}, models = [ \"Table\" ], )","title":"query()"},{"location":"query/api-reference/#get","text":"Retrieve documents by ID or filter from multiple collections. async def get ( * , ids : Optional [ Sequence [ str ]] = None , where : Optional [ Mapping [ str , Any ]] = None , where_document : Optional [ WhereDocument ] = None , limit : Optional [ int ] = None , offset : Optional [ int ] = None , include : Optional [ Include ] = None , models : Optional [ Sequence [ str ]] = None , ) -> Dict [ str , Any ] Parameters: ids (Sequence[str], optional): Document IDs to retrieve where (Mapping[str, Any], optional): Metadata filter where_document (WhereDocument, optional): Document content filter limit (int, optional): Maximum number of documents to return offset (int, optional): Number of documents to skip include (Include, optional): Fields to include (default: [\"metadatas\", \"documents\"]) models (Sequence[str], optional): Model names to query Returns: Dictionary containing ids, documents, metadatas, etc. Example: # Get by IDs docs = await client . get ( ids = [ \"doc_123\" , \"doc_456\" ], models = [ \"Table\" ], ) # Get with filter docs = await client . get ( where = { \"has_sem\" : True }, limit = 100 , models = None , )","title":"get()"},{"location":"query/api-reference/#count","text":"Count total documents across collections. async def count ( * , where : Optional [ Mapping [ str , Any ]] = None , models : Optional [ Sequence [ str ]] = None , ) -> int Parameters: where (Mapping[str, Any], optional): Metadata filter models (Sequence[str], optional): Model names to count Returns: Total document count across queried collections Example: # Count all documents total = await client . count ( models = None ) # Count specific model table_count = await client . count ( models = [ \"Table\" ]) # Count with filter sem_count = await client . count ( where = { \"has_sem\" : True }, models = None , )","title":"count()"},{"location":"query/api-reference/#connect-close","text":"Manually manage connection lifecycle (typically use context manager instead). async def connect () -> None async def close () -> None Example: # Manual lifecycle management client = AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" )) await client . connect () try : results = await client . query ( query_texts = [ \"search\" ], n_results = 10 ) finally : await client . close () # Recommended: use context manager async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" )) as client : results = await client . query ( query_texts = [ \"search\" ], n_results = 10 )","title":"connect() / close()"},{"location":"query/api-reference/#query-config-functions","text":"","title":"Query Config Functions"},{"location":"query/api-reference/#generate_query_config","text":"Generate query configuration from indexed partitions. def generate_query_config ( partition_out_dir : Path , * , output_path : Optional [ Path ] = None , collection_prefix : Optional [ str ] = None , ) -> Dict [ str , Any ] Parameters: partition_out_dir (Path): Directory containing partition subdirectories with resume state files output_path (Path, optional): Path to write query config JSON (if not provided, returns dict only) collection_prefix (str, optional): Optional prefix added to collection names Returns: Query configuration dictionary Example: from indexer.vectorize_lib import generate_query_config from pathlib import Path config = generate_query_config ( partition_out_dir = Path ( \"build/vector\" ), output_path = Path ( \"query_config.json\" ), collection_prefix = \"ecc-prod\" , ) print ( f \"Generated config with { config [ 'metadata' ][ 'total_models' ] } models\" )","title":"generate_query_config()"},{"location":"query/api-reference/#load_query_config","text":"Load and validate query configuration from JSON file. def load_query_config ( config_path : Path ) -> Dict [ str , Any ] Parameters: config_path (Path): Path to query config JSON file Returns: Query configuration dictionary Raises: ValueError : If file doesn't exist, is invalid JSON, or missing required keys Example: from indexer.vectorize_lib import load_query_config from pathlib import Path config = load_query_config ( Path ( \"query_config.json\" )) print ( f \"Loaded config with { len ( config [ 'model_to_collections' ]) } models\" )","title":"load_query_config()"},{"location":"query/api-reference/#get_collections_for_models","text":"Get list of collections to query for given model names. def get_collections_for_models ( query_config : Dict [ str , Any ], model_names : Optional [ List [ str ]] = None , ) -> List [ str ] Parameters: query_config (Dict[str, Any]): Query configuration dictionary model_names (List[str], optional): Model names to query. If None, returns all collections Returns: List of collection names Example: from indexer.vectorize_lib import load_query_config , get_collections_for_models from pathlib import Path config = load_query_config ( Path ( \"query_config.json\" )) # Get collections for specific models collections = get_collections_for_models ( config , [ \"Table\" , \"Field\" ]) print ( f \"Collections: { collections } \" ) # [\"partition_00001\", \"partition_00002\", ...] # Get all collections all_collections = get_collections_for_models ( config , None ) print ( f \"All collections: { all_collections } \" )","title":"get_collections_for_models()"},{"location":"query/api-reference/#chromadb-metadata-filters","text":"The where parameter supports ChromaDB's metadata filtering operators: # Equality where = { \"model_name\" : \"Table\" } # Comparison where = { \"schema_version\" : { \"$gte\" : 2 }} # In/Not In where = { \"model_name\" : { \"$in\" : [ \"Table\" , \"Field\" ]}} # Logical operators where = { \"$and\" : [ { \"has_sem\" : True }, { \"schema_version\" : { \"$gte\" : 2 }} ] } Supported operators: $eq : Equal $ne : Not equal $gt : Greater than $gte : Greater than or equal $lt : Less than $lte : Less than or equal $in : In list $nin : Not in list $and : Logical AND $or : Logical OR See ChromaDB metadata filtering documentation for more details.","title":"ChromaDB Metadata Filters"},{"location":"query/config/","text":"Query Configuration \u00b6 The query configuration file maps model names to ChromaDB collection names, enabling efficient multi-collection queries. Generating Query Config \u00b6 CLI Command \u00b6 idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json \\ --model path/to/model_registry.yaml \\ --collection-prefix my-index Arguments: --partition-out-dir (required): Directory containing partition subdirectories with resume state files --output (required): Path to write query config JSON file --model (required): Python module path to model registry (e.g., my_package.registry:MODEL_REGISTRY ) --collection-prefix (optional): Prefix to add to collection names Python API \u00b6 from indexer.vectorize_lib import generate_query_config from pathlib import Path config = generate_query_config ( partition_out_dir = Path ( \"build/vector\" ), output_path = Path ( \"query_config.json\" ), collection_prefix = \"ecc-prod\" , ) Config File Structure \u00b6 Full Example \u00b6 { \"metadata\" : { \"generated_at\" : \"2025-10-31T10:00:00.123456\" , \"total_models\" : 3 , \"total_collections\" : 5 , \"collection_prefix\" : \"ecc-prod\" }, \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"partition_00001\" , \"partition_00002\" ], \"total_documents\" : 450000 , \"partitions\" : [ \"partition_00001\" , \"partition_00002\" ] }, \"Field\" : { \"collections\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ], \"total_documents\" : 680000 , \"partitions\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ] }, \"Domain\" : { \"collections\" : [ \"partition_00004\" , \"partition_00005\" ], \"total_documents\" : 120000 , \"partitions\" : [ \"partition_00004\" , \"partition_00005\" ] } }, \"collection_to_models\" : { \"partition_00001\" : [ \"Table\" ], \"partition_00002\" : [ \"Table\" , \"Field\" ], \"partition_00003\" : [ \"Field\" ], \"partition_00004\" : [ \"Domain\" ], \"partition_00005\" : [ \"Field\" , \"Domain\" ] } } Field Descriptions \u00b6 metadata \u00b6 generated_at : ISO 8601 timestamp of config generation total_models : Number of distinct models with indexed documents total_collections : Number of ChromaDB collections (partitions) collection_prefix : Optional prefix applied to collection names model_to_collections \u00b6 Mapping from model name to collection information: collections : List of collection names containing this model total_documents : Total document count across all collections for this model partitions : List of partition names (same as collections unless prefix is used) collection_to_models \u00b6 Inverse mapping from collection name to list of model names it contains. Resume State Requirements \u00b6 The query config is generated by scanning *_resume_state.json files in each partition directory. These files must contain: { \"ModelName\" : { \"started\" : true , \"complete\" : true , \"collection_count\" : 100000 , \"documents_indexed\" : 100000 , \"indexed_at\" : \"2025-10-31T10:00:00\" } } Required fields: started : Must be true (models not started are excluded) collection_count : Must be > 0 (models with no documents are excluded) Optional fields: complete : Indicates if indexing completed (doesn't affect inclusion) documents_indexed : Actual documents indexed (may differ from collection_count) indexed_at : Timestamp of indexing Collection Filtering Logic \u00b6 When generating the query config: Scan partition directories for *_resume_state.json files Filter models where started == true and collection_count > 0 Build mappings for model-to-collections and collection-to-models Sort collections for deterministic ordering Excluded Models \u00b6 Models are excluded if: started is false or missing collection_count is 0 or missing Resume state file is malformed Model state is not a dictionary Using Collection Prefix \u00b6 When indexing with a collection prefix, ensure query config uses the same prefix: Indexing with Prefix \u00b6 idxr vectorize index \\ --collection-prefix ecc-prod \\ # ... other args Query Config with Prefix \u00b6 idxr vectorize generate-query-config \\ --collection-prefix ecc-prod \\ # ... other args The prefix is applied to collection names in the config: { \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"ecc-prod_partition_00001\" , \"ecc-prod_partition_00002\" ] } } } Config Validation \u00b6 When loading a query config, the following validations are performed: File exists and is readable Valid JSON format Required keys present: metadata , model_to_collections , collection_to_models Metadata fields include total_models and total_collections Validation Errors \u00b6 from indexer.vectorize_lib import load_query_config from pathlib import Path try : config = load_query_config ( Path ( \"query_config.json\" )) except ValueError as e : # Possible errors: # - \"Query config file <path> does not exist\" # - \"Failed to read query config: <json error>\" # - \"Query config is missing required keys: ...\" print ( f \"Config error: { e } \" ) Advanced Configuration \u00b6 Multi-Environment Configs \u00b6 Generate separate configs for different environments: # Production index idxr vectorize generate-query-config \\ --partition-out-dir prod/vector \\ --output query_config_prod.json \\ --collection-prefix prod # Staging index idxr vectorize generate-query-config \\ --partition-out-dir staging/vector \\ --output query_config_staging.json \\ --collection-prefix staging Partial Model Indexing \u00b6 If you index models at different times, regenerate the query config after each indexing batch: # After indexing Table model idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json # After adding Field model # Regenerate to include Field in the config idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json The config will automatically include all models that have been indexed. Version Control \u00b6 Store query configs in version control alongside model registries: project / \u251c\u2500\u2500 config / \u2502 \u251c\u2500\u2500 model_registry . yaml \u2502 \u251c\u2500\u2500 query_config_prod . json \u2502 \u2514\u2500\u2500 query_config_staging . json \u251c\u2500\u2500 src / \u2514\u2500\u2500 README . md Update the config after indexing changes: # After re-indexing idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output config/query_config_prod.json # Commit changes git add config/query_config_prod.json git commit -m \"Update query config after re-indexing\" Troubleshooting \u00b6 No Collections Found \u00b6 Problem : Query config has total_collections: 0 Causes : - No resume state files in partition directories - All models have started: false - All models have collection_count: 0 Solution : # Check partition directory structure ls -la build/vector/partition_*/ # Check resume state content cat build/vector/partition_00001/partition_00001_resume_state.json Model Not in Config \u00b6 Problem : Model exists in resume state but not in query config Causes : - Model has started: false - Model has collection_count: 0 - Resume state file is malformed Solution : import json from pathlib import Path # Check resume state resume_file = Path ( \"build/vector/partition_00001/partition_00001_resume_state.json\" ) state = json . loads ( resume_file . read_text ()) for model , info in state . items (): print ( f \" { model } :\" ) print ( f \" started: { info . get ( 'started' ) } \" ) print ( f \" collection_count: { info . get ( 'collection_count' ) } \" ) Collection Count Mismatch \u00b6 Problem : Config shows fewer collections than expected Causes : - Some partitions have no valid models (excluded from config) - Collection prefix mismatch Solution : # Verify all partitions have resume states find build/vector -name \"*_resume_state.json\" # Check for empty or invalid states for f in build/vector/partition_*/*_resume_state.json ; do echo \"=== $f ===\" cat \" $f \" done Next Steps \u00b6 Review API Reference for programmatic config access Check Examples for query config usage patterns Read Getting Started for complete workflow","title":"Config Reference"},{"location":"query/config/#query-configuration","text":"The query configuration file maps model names to ChromaDB collection names, enabling efficient multi-collection queries.","title":"Query Configuration"},{"location":"query/config/#generating-query-config","text":"","title":"Generating Query Config"},{"location":"query/config/#cli-command","text":"idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json \\ --model path/to/model_registry.yaml \\ --collection-prefix my-index Arguments: --partition-out-dir (required): Directory containing partition subdirectories with resume state files --output (required): Path to write query config JSON file --model (required): Python module path to model registry (e.g., my_package.registry:MODEL_REGISTRY ) --collection-prefix (optional): Prefix to add to collection names","title":"CLI Command"},{"location":"query/config/#python-api","text":"from indexer.vectorize_lib import generate_query_config from pathlib import Path config = generate_query_config ( partition_out_dir = Path ( \"build/vector\" ), output_path = Path ( \"query_config.json\" ), collection_prefix = \"ecc-prod\" , )","title":"Python API"},{"location":"query/config/#config-file-structure","text":"","title":"Config File Structure"},{"location":"query/config/#full-example","text":"{ \"metadata\" : { \"generated_at\" : \"2025-10-31T10:00:00.123456\" , \"total_models\" : 3 , \"total_collections\" : 5 , \"collection_prefix\" : \"ecc-prod\" }, \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"partition_00001\" , \"partition_00002\" ], \"total_documents\" : 450000 , \"partitions\" : [ \"partition_00001\" , \"partition_00002\" ] }, \"Field\" : { \"collections\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ], \"total_documents\" : 680000 , \"partitions\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ] }, \"Domain\" : { \"collections\" : [ \"partition_00004\" , \"partition_00005\" ], \"total_documents\" : 120000 , \"partitions\" : [ \"partition_00004\" , \"partition_00005\" ] } }, \"collection_to_models\" : { \"partition_00001\" : [ \"Table\" ], \"partition_00002\" : [ \"Table\" , \"Field\" ], \"partition_00003\" : [ \"Field\" ], \"partition_00004\" : [ \"Domain\" ], \"partition_00005\" : [ \"Field\" , \"Domain\" ] } }","title":"Full Example"},{"location":"query/config/#field-descriptions","text":"","title":"Field Descriptions"},{"location":"query/config/#metadata","text":"generated_at : ISO 8601 timestamp of config generation total_models : Number of distinct models with indexed documents total_collections : Number of ChromaDB collections (partitions) collection_prefix : Optional prefix applied to collection names","title":"metadata"},{"location":"query/config/#model_to_collections","text":"Mapping from model name to collection information: collections : List of collection names containing this model total_documents : Total document count across all collections for this model partitions : List of partition names (same as collections unless prefix is used)","title":"model_to_collections"},{"location":"query/config/#collection_to_models","text":"Inverse mapping from collection name to list of model names it contains.","title":"collection_to_models"},{"location":"query/config/#resume-state-requirements","text":"The query config is generated by scanning *_resume_state.json files in each partition directory. These files must contain: { \"ModelName\" : { \"started\" : true , \"complete\" : true , \"collection_count\" : 100000 , \"documents_indexed\" : 100000 , \"indexed_at\" : \"2025-10-31T10:00:00\" } } Required fields: started : Must be true (models not started are excluded) collection_count : Must be > 0 (models with no documents are excluded) Optional fields: complete : Indicates if indexing completed (doesn't affect inclusion) documents_indexed : Actual documents indexed (may differ from collection_count) indexed_at : Timestamp of indexing","title":"Resume State Requirements"},{"location":"query/config/#collection-filtering-logic","text":"When generating the query config: Scan partition directories for *_resume_state.json files Filter models where started == true and collection_count > 0 Build mappings for model-to-collections and collection-to-models Sort collections for deterministic ordering","title":"Collection Filtering Logic"},{"location":"query/config/#excluded-models","text":"Models are excluded if: started is false or missing collection_count is 0 or missing Resume state file is malformed Model state is not a dictionary","title":"Excluded Models"},{"location":"query/config/#using-collection-prefix","text":"When indexing with a collection prefix, ensure query config uses the same prefix:","title":"Using Collection Prefix"},{"location":"query/config/#indexing-with-prefix","text":"idxr vectorize index \\ --collection-prefix ecc-prod \\ # ... other args","title":"Indexing with Prefix"},{"location":"query/config/#query-config-with-prefix","text":"idxr vectorize generate-query-config \\ --collection-prefix ecc-prod \\ # ... other args The prefix is applied to collection names in the config: { \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"ecc-prod_partition_00001\" , \"ecc-prod_partition_00002\" ] } } }","title":"Query Config with Prefix"},{"location":"query/config/#config-validation","text":"When loading a query config, the following validations are performed: File exists and is readable Valid JSON format Required keys present: metadata , model_to_collections , collection_to_models Metadata fields include total_models and total_collections","title":"Config Validation"},{"location":"query/config/#validation-errors","text":"from indexer.vectorize_lib import load_query_config from pathlib import Path try : config = load_query_config ( Path ( \"query_config.json\" )) except ValueError as e : # Possible errors: # - \"Query config file <path> does not exist\" # - \"Failed to read query config: <json error>\" # - \"Query config is missing required keys: ...\" print ( f \"Config error: { e } \" )","title":"Validation Errors"},{"location":"query/config/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"query/config/#multi-environment-configs","text":"Generate separate configs for different environments: # Production index idxr vectorize generate-query-config \\ --partition-out-dir prod/vector \\ --output query_config_prod.json \\ --collection-prefix prod # Staging index idxr vectorize generate-query-config \\ --partition-out-dir staging/vector \\ --output query_config_staging.json \\ --collection-prefix staging","title":"Multi-Environment Configs"},{"location":"query/config/#partial-model-indexing","text":"If you index models at different times, regenerate the query config after each indexing batch: # After indexing Table model idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json # After adding Field model # Regenerate to include Field in the config idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json The config will automatically include all models that have been indexed.","title":"Partial Model Indexing"},{"location":"query/config/#version-control","text":"Store query configs in version control alongside model registries: project / \u251c\u2500\u2500 config / \u2502 \u251c\u2500\u2500 model_registry . yaml \u2502 \u251c\u2500\u2500 query_config_prod . json \u2502 \u2514\u2500\u2500 query_config_staging . json \u251c\u2500\u2500 src / \u2514\u2500\u2500 README . md Update the config after indexing changes: # After re-indexing idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output config/query_config_prod.json # Commit changes git add config/query_config_prod.json git commit -m \"Update query config after re-indexing\"","title":"Version Control"},{"location":"query/config/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"query/config/#no-collections-found","text":"Problem : Query config has total_collections: 0 Causes : - No resume state files in partition directories - All models have started: false - All models have collection_count: 0 Solution : # Check partition directory structure ls -la build/vector/partition_*/ # Check resume state content cat build/vector/partition_00001/partition_00001_resume_state.json","title":"No Collections Found"},{"location":"query/config/#model-not-in-config","text":"Problem : Model exists in resume state but not in query config Causes : - Model has started: false - Model has collection_count: 0 - Resume state file is malformed Solution : import json from pathlib import Path # Check resume state resume_file = Path ( \"build/vector/partition_00001/partition_00001_resume_state.json\" ) state = json . loads ( resume_file . read_text ()) for model , info in state . items (): print ( f \" { model } :\" ) print ( f \" started: { info . get ( 'started' ) } \" ) print ( f \" collection_count: { info . get ( 'collection_count' ) } \" )","title":"Model Not in Config"},{"location":"query/config/#collection-count-mismatch","text":"Problem : Config shows fewer collections than expected Causes : - Some partitions have no valid models (excluded from config) - Collection prefix mismatch Solution : # Verify all partitions have resume states find build/vector -name \"*_resume_state.json\" # Check for empty or invalid states for f in build/vector/partition_*/*_resume_state.json ; do echo \"=== $f ===\" cat \" $f \" done","title":"Collection Count Mismatch"},{"location":"query/config/#next-steps","text":"Review API Reference for programmatic config access Check Examples for query config usage patterns Read Getting Started for complete workflow","title":"Next Steps"},{"location":"query/examples/","text":"Query Examples \u00b6 Practical examples for common query patterns with AsyncMultiCollectionQueryClient. Basic Queries \u00b6 Query All Models \u00b6 Search across all collections when you don't know which model contains the answer: from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import asyncio async def query_all (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ \"What are SAP authorization objects?\" ], n_results = 10 , models = None , # Query ALL collections ) print ( f \"Found { len ( results [ 'ids' ][ 0 ]) } results:\" ) for doc_id , distance , metadata in zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): model = metadata . get ( \"model_name\" , \"unknown\" ) print ( f \" [ { model } ] { doc_id } - Distance: { distance : .4f } \" ) asyncio . run ( query_all ()) Query Specific Models \u00b6 Search only relevant collections when you know the model: async def query_specific (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ \"transaction table MARA\" ], n_results = 10 , models = [ \"Table\" ], # Only Table collections ) for doc_id in results [ \"ids\" ][ 0 ]: print ( f \" { doc_id } \" ) asyncio . run ( query_specific ()) Advanced Filtering \u00b6 Query with Metadata Filters \u00b6 Combine model filtering with metadata constraints: async def query_with_filters (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : # Only semantic-enabled records results = await client . query ( query_texts = [ \"customer data\" ], n_results = 10 , where = { \"has_sem\" : True }, models = [ \"Table\" , \"Field\" ], ) # Complex filter results = await client . query ( query_texts = [ \"sales data\" ], n_results = 10 , where = { \"$and\" : [ { \"has_sem\" : True }, { \"schema_version\" : { \"$gte\" : 2 }}, { \"model_name\" : { \"$in\" : [ \"Table\" , \"View\" ]}} ] }, models = None , ) asyncio . run ( query_with_filters ()) Batch Queries \u00b6 Query Multiple Texts in Parallel \u00b6 Query multiple search terms simultaneously: async def batch_query (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : # Query 3 different texts at once results = await client . query ( query_texts = [ \"SAP authorization objects\" , \"transaction tables\" , \"customer master data\" ], n_results = 5 , models = None , ) # Process results for each query for query_idx , query_text in enumerate ([ \"SAP authorization objects\" , \"transaction tables\" , \"customer master data\" ]): print ( f \" \\n Results for: { query_text } \" ) for doc_id , distance in zip ( results [ \"ids\" ][ query_idx ], results [ \"distances\" ][ query_idx ] ): print ( f \" { doc_id } - { distance : .4f } \" ) asyncio . run ( batch_query ()) Document Retrieval \u00b6 Get Documents by ID \u00b6 Retrieve specific documents from collections: async def get_by_id (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : docs = await client . get ( ids = [ \"Table_MARA_001\" , \"Field_MATNR_001\" ], models = [ \"Table\" , \"Field\" ], ) for doc_id , doc_text , metadata in zip ( docs [ \"ids\" ], docs [ \"documents\" ], docs [ \"metadatas\" ] ): print ( f \" { doc_id } :\" ) print ( f \" Text: { doc_text [: 100 ] } ...\" ) print ( f \" Metadata: { metadata } \" ) asyncio . run ( get_by_id ()) Get Documents with Filter \u00b6 Retrieve documents matching criteria: async def get_with_filter (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : docs = await client . get ( where = { \"has_sem\" : True , \"model_name\" : \"Table\" }, limit = 100 , models = None , ) print ( f \"Retrieved { len ( docs [ 'ids' ]) } documents\" ) asyncio . run ( get_with_filter ()) Counting \u00b6 Count Documents \u00b6 Get document counts across collections: async def count_documents (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : # Count all documents total = await client . count ( models = None ) print ( f \"Total documents: { total : , } \" ) # Count by model table_count = await client . count ( models = [ \"Table\" ]) print ( f \"Table documents: { table_count : , } \" ) # Count with filter sem_count = await client . count ( where = { \"has_sem\" : True }, models = None , ) print ( f \"Semantic-enabled documents: { sem_count : , } \" ) asyncio . run ( count_documents ()) RAG Pipeline Integration \u00b6 Semantic Search for RAG \u00b6 Integrate with RAG (Retrieval-Augmented Generation) pipeline: import openai from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path async def rag_search ( user_question : str ) -> str : \"\"\"Search index and generate answer using RAG.\"\"\" # Step 1: Retrieve relevant documents async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ user_question ], n_results = 5 , models = None , ) # Step 2: Extract context from results context_docs = [] for doc_text , metadata in zip ( results [ \"documents\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): model = metadata . get ( \"model_name\" , \"unknown\" ) context_docs . append ( f \"[ { model } ] { doc_text } \" ) context = \" \\n\\n \" . join ( context_docs ) # Step 3: Generate answer with LLM response = openai . chat . completions . create ( model = \"gpt-4\" , messages = [ { \"role\" : \"system\" , \"content\" : \"Answer questions based on the provided context.\" }, { \"role\" : \"user\" , \"content\" : f \"Context: \\n { context } \\n\\n Question: { user_question } \" } ] ) return response . choices [ 0 ] . message . content # Usage answer = asyncio . run ( rag_search ( \"What are SAP authorization objects?\" )) print ( answer ) Error Handling \u00b6 Graceful Degradation \u00b6 Handle partial collection failures: async def robust_query (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : try : results = await client . query ( query_texts = [ \"search term\" ], n_results = 10 , models = None , ) # Check if we got results if len ( results [ \"ids\" ][ 0 ]) == 0 : print ( \"No results found\" ) else : print ( f \"Found { len ( results [ 'ids' ][ 0 ]) } results\" ) except RuntimeError as e : print ( f \"Client error: { e } \" ) except ValueError as e : print ( f \"Invalid query: { e } \" ) asyncio . run ( robust_query ()) Performance Optimization \u00b6 Model-Specific Queries \u00b6 Optimize performance by querying only relevant models: async def optimized_query ( query_text : str , intent : str ): \"\"\"Route queries to relevant models based on intent.\"\"\" # Map intents to models intent_to_models = { \"table_lookup\" : [ \"Table\" ], \"field_search\" : [ \"Field\" ], \"authorization\" : [ \"AuthObject\" , \"Function\" ], \"general\" : None , # Query all } models = intent_to_models . get ( intent , None ) async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ query_text ], n_results = 10 , models = models , ) return results # Fast lookup in Table model only results = asyncio . run ( optimized_query ( \"MARA table\" , \"table_lookup\" )) # Broad search across all models results = asyncio . run ( optimized_query ( \"SAP authorization\" , \"general\" )) Complete Application Example \u00b6 Full-Featured Search Application \u00b6 from indexer.vectorize_lib import ( AsyncMultiCollectionQueryClient , generate_query_config , ) from pathlib import Path import os import asyncio from typing import Optional , List class SAPIndexSearch : \"\"\"Wrapper for SAP index search functionality.\"\"\" def __init__ ( self , config_path : Path , api_key : str ): self . config_path = config_path self . api_key = api_key self . client : Optional [ AsyncMultiCollectionQueryClient ] = None async def __aenter__ ( self ): self . client = AsyncMultiCollectionQueryClient ( config_path = self . config_path , client_type = \"cloud\" , cloud_api_key = self . api_key , ) await self . client . connect () return self async def __aexit__ ( self , * args ): if self . client : await self . client . close () async def search ( self , query : str , models : Optional [ List [ str ]] = None , filters : Optional [ dict ] = None , limit : int = 10 ) -> List [ dict ]: \"\"\"Search with automatic model routing.\"\"\" results = await self . client . query ( query_texts = [ query ], n_results = limit , where = filters , models = models , ) # Format results formatted = [] for doc_id , distance , doc_text , metadata in zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"documents\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): formatted . append ({ \"id\" : doc_id , \"score\" : 1 - distance , # Convert to similarity \"text\" : doc_text , \"model\" : metadata . get ( \"model_name\" ), \"metadata\" : metadata , }) return formatted async def get_stats ( self ) -> dict : \"\"\"Get index statistics.\"\"\" total = await self . client . count ( models = None ) # Count per model (example with known models) models = [ \"Table\" , \"Field\" , \"Function\" ] model_counts = {} for model in models : try : count = await self . client . count ( models = [ model ]) model_counts [ model ] = count except : pass return { \"total_documents\" : total , \"model_counts\" : model_counts , } # Usage async def main (): async with SAPIndexSearch ( config_path = Path ( \"query_config.json\" ), api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as search : # Search results = search . await search ( \"SAP authorization objects\" , models = None , limit = 5 ) for result in results : print ( f \"[ { result [ 'model' ] } ] Score: { result [ 'score' ] : .4f } \" ) print ( f \" { result [ 'text' ][: 100 ] } ...\" ) # Stats stats = await search . get_stats () print ( f \" \\n Index Statistics:\" ) print ( f \" Total: { stats [ 'total_documents' ] : , } \" ) for model , count in stats [ 'model_counts' ] . items (): print ( f \" { model } : { count : , } \" ) asyncio . run ( main ()) Next Steps \u00b6 Review API Reference for complete method documentation Check Best Practices for performance tips Read Configuration for query config options","title":"Examples"},{"location":"query/examples/#query-examples","text":"Practical examples for common query patterns with AsyncMultiCollectionQueryClient.","title":"Query Examples"},{"location":"query/examples/#basic-queries","text":"","title":"Basic Queries"},{"location":"query/examples/#query-all-models","text":"Search across all collections when you don't know which model contains the answer: from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import asyncio async def query_all (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ \"What are SAP authorization objects?\" ], n_results = 10 , models = None , # Query ALL collections ) print ( f \"Found { len ( results [ 'ids' ][ 0 ]) } results:\" ) for doc_id , distance , metadata in zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): model = metadata . get ( \"model_name\" , \"unknown\" ) print ( f \" [ { model } ] { doc_id } - Distance: { distance : .4f } \" ) asyncio . run ( query_all ())","title":"Query All Models"},{"location":"query/examples/#query-specific-models","text":"Search only relevant collections when you know the model: async def query_specific (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ \"transaction table MARA\" ], n_results = 10 , models = [ \"Table\" ], # Only Table collections ) for doc_id in results [ \"ids\" ][ 0 ]: print ( f \" { doc_id } \" ) asyncio . run ( query_specific ())","title":"Query Specific Models"},{"location":"query/examples/#advanced-filtering","text":"","title":"Advanced Filtering"},{"location":"query/examples/#query-with-metadata-filters","text":"Combine model filtering with metadata constraints: async def query_with_filters (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : # Only semantic-enabled records results = await client . query ( query_texts = [ \"customer data\" ], n_results = 10 , where = { \"has_sem\" : True }, models = [ \"Table\" , \"Field\" ], ) # Complex filter results = await client . query ( query_texts = [ \"sales data\" ], n_results = 10 , where = { \"$and\" : [ { \"has_sem\" : True }, { \"schema_version\" : { \"$gte\" : 2 }}, { \"model_name\" : { \"$in\" : [ \"Table\" , \"View\" ]}} ] }, models = None , ) asyncio . run ( query_with_filters ())","title":"Query with Metadata Filters"},{"location":"query/examples/#batch-queries","text":"","title":"Batch Queries"},{"location":"query/examples/#query-multiple-texts-in-parallel","text":"Query multiple search terms simultaneously: async def batch_query (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : # Query 3 different texts at once results = await client . query ( query_texts = [ \"SAP authorization objects\" , \"transaction tables\" , \"customer master data\" ], n_results = 5 , models = None , ) # Process results for each query for query_idx , query_text in enumerate ([ \"SAP authorization objects\" , \"transaction tables\" , \"customer master data\" ]): print ( f \" \\n Results for: { query_text } \" ) for doc_id , distance in zip ( results [ \"ids\" ][ query_idx ], results [ \"distances\" ][ query_idx ] ): print ( f \" { doc_id } - { distance : .4f } \" ) asyncio . run ( batch_query ())","title":"Query Multiple Texts in Parallel"},{"location":"query/examples/#document-retrieval","text":"","title":"Document Retrieval"},{"location":"query/examples/#get-documents-by-id","text":"Retrieve specific documents from collections: async def get_by_id (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : docs = await client . get ( ids = [ \"Table_MARA_001\" , \"Field_MATNR_001\" ], models = [ \"Table\" , \"Field\" ], ) for doc_id , doc_text , metadata in zip ( docs [ \"ids\" ], docs [ \"documents\" ], docs [ \"metadatas\" ] ): print ( f \" { doc_id } :\" ) print ( f \" Text: { doc_text [: 100 ] } ...\" ) print ( f \" Metadata: { metadata } \" ) asyncio . run ( get_by_id ())","title":"Get Documents by ID"},{"location":"query/examples/#get-documents-with-filter","text":"Retrieve documents matching criteria: async def get_with_filter (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : docs = await client . get ( where = { \"has_sem\" : True , \"model_name\" : \"Table\" }, limit = 100 , models = None , ) print ( f \"Retrieved { len ( docs [ 'ids' ]) } documents\" ) asyncio . run ( get_with_filter ())","title":"Get Documents with Filter"},{"location":"query/examples/#counting","text":"","title":"Counting"},{"location":"query/examples/#count-documents","text":"Get document counts across collections: async def count_documents (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : # Count all documents total = await client . count ( models = None ) print ( f \"Total documents: { total : , } \" ) # Count by model table_count = await client . count ( models = [ \"Table\" ]) print ( f \"Table documents: { table_count : , } \" ) # Count with filter sem_count = await client . count ( where = { \"has_sem\" : True }, models = None , ) print ( f \"Semantic-enabled documents: { sem_count : , } \" ) asyncio . run ( count_documents ())","title":"Count Documents"},{"location":"query/examples/#rag-pipeline-integration","text":"","title":"RAG Pipeline Integration"},{"location":"query/examples/#semantic-search-for-rag","text":"Integrate with RAG (Retrieval-Augmented Generation) pipeline: import openai from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path async def rag_search ( user_question : str ) -> str : \"\"\"Search index and generate answer using RAG.\"\"\" # Step 1: Retrieve relevant documents async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ user_question ], n_results = 5 , models = None , ) # Step 2: Extract context from results context_docs = [] for doc_text , metadata in zip ( results [ \"documents\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): model = metadata . get ( \"model_name\" , \"unknown\" ) context_docs . append ( f \"[ { model } ] { doc_text } \" ) context = \" \\n\\n \" . join ( context_docs ) # Step 3: Generate answer with LLM response = openai . chat . completions . create ( model = \"gpt-4\" , messages = [ { \"role\" : \"system\" , \"content\" : \"Answer questions based on the provided context.\" }, { \"role\" : \"user\" , \"content\" : f \"Context: \\n { context } \\n\\n Question: { user_question } \" } ] ) return response . choices [ 0 ] . message . content # Usage answer = asyncio . run ( rag_search ( \"What are SAP authorization objects?\" )) print ( answer )","title":"Semantic Search for RAG"},{"location":"query/examples/#error-handling","text":"","title":"Error Handling"},{"location":"query/examples/#graceful-degradation","text":"Handle partial collection failures: async def robust_query (): async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : try : results = await client . query ( query_texts = [ \"search term\" ], n_results = 10 , models = None , ) # Check if we got results if len ( results [ \"ids\" ][ 0 ]) == 0 : print ( \"No results found\" ) else : print ( f \"Found { len ( results [ 'ids' ][ 0 ]) } results\" ) except RuntimeError as e : print ( f \"Client error: { e } \" ) except ValueError as e : print ( f \"Invalid query: { e } \" ) asyncio . run ( robust_query ())","title":"Graceful Degradation"},{"location":"query/examples/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"query/examples/#model-specific-queries","text":"Optimize performance by querying only relevant models: async def optimized_query ( query_text : str , intent : str ): \"\"\"Route queries to relevant models based on intent.\"\"\" # Map intents to models intent_to_models = { \"table_lookup\" : [ \"Table\" ], \"field_search\" : [ \"Field\" ], \"authorization\" : [ \"AuthObject\" , \"Function\" ], \"general\" : None , # Query all } models = intent_to_models . get ( intent , None ) async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = \"your-api-key\" , ) as client : results = await client . query ( query_texts = [ query_text ], n_results = 10 , models = models , ) return results # Fast lookup in Table model only results = asyncio . run ( optimized_query ( \"MARA table\" , \"table_lookup\" )) # Broad search across all models results = asyncio . run ( optimized_query ( \"SAP authorization\" , \"general\" ))","title":"Model-Specific Queries"},{"location":"query/examples/#complete-application-example","text":"","title":"Complete Application Example"},{"location":"query/examples/#full-featured-search-application","text":"from indexer.vectorize_lib import ( AsyncMultiCollectionQueryClient , generate_query_config , ) from pathlib import Path import os import asyncio from typing import Optional , List class SAPIndexSearch : \"\"\"Wrapper for SAP index search functionality.\"\"\" def __init__ ( self , config_path : Path , api_key : str ): self . config_path = config_path self . api_key = api_key self . client : Optional [ AsyncMultiCollectionQueryClient ] = None async def __aenter__ ( self ): self . client = AsyncMultiCollectionQueryClient ( config_path = self . config_path , client_type = \"cloud\" , cloud_api_key = self . api_key , ) await self . client . connect () return self async def __aexit__ ( self , * args ): if self . client : await self . client . close () async def search ( self , query : str , models : Optional [ List [ str ]] = None , filters : Optional [ dict ] = None , limit : int = 10 ) -> List [ dict ]: \"\"\"Search with automatic model routing.\"\"\" results = await self . client . query ( query_texts = [ query ], n_results = limit , where = filters , models = models , ) # Format results formatted = [] for doc_id , distance , doc_text , metadata in zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"documents\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): formatted . append ({ \"id\" : doc_id , \"score\" : 1 - distance , # Convert to similarity \"text\" : doc_text , \"model\" : metadata . get ( \"model_name\" ), \"metadata\" : metadata , }) return formatted async def get_stats ( self ) -> dict : \"\"\"Get index statistics.\"\"\" total = await self . client . count ( models = None ) # Count per model (example with known models) models = [ \"Table\" , \"Field\" , \"Function\" ] model_counts = {} for model in models : try : count = await self . client . count ( models = [ model ]) model_counts [ model ] = count except : pass return { \"total_documents\" : total , \"model_counts\" : model_counts , } # Usage async def main (): async with SAPIndexSearch ( config_path = Path ( \"query_config.json\" ), api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as search : # Search results = search . await search ( \"SAP authorization objects\" , models = None , limit = 5 ) for result in results : print ( f \"[ { result [ 'model' ] } ] Score: { result [ 'score' ] : .4f } \" ) print ( f \" { result [ 'text' ][: 100 ] } ...\" ) # Stats stats = await search . get_stats () print ( f \" \\n Index Statistics:\" ) print ( f \" Total: { stats [ 'total_documents' ] : , } \" ) for model , count in stats [ 'model_counts' ] . items (): print ( f \" { model } : { count : , } \" ) asyncio . run ( main ())","title":"Full-Featured Search Application"},{"location":"query/examples/#next-steps","text":"Review API Reference for complete method documentation Check Best Practices for performance tips Read Configuration for query config options","title":"Next Steps"},{"location":"query/getting-started/","text":"Getting Started with Query \u00b6 This guide shows you how to query multi-collection ChromaDB indexes created with idxr. Prerequisites \u00b6 Indexed data : You must have already indexed data using idxr vectorize index Python environment : Python 3.9+ with idxr installed ChromaDB access : Connection details for ChromaDB (local or cloud) Installation \u00b6 The query client is included with the idxr package: pip install idxr Basic Workflow \u00b6 Step 1: Generate Query Config \u00b6 After indexing completes, generate a query configuration: idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json \\ --model path/to/model_registry.yaml This scans all *_resume_state.json files in your partition directory and creates a mapping of model names to collection names. Output ( query_config.json ): { \"metadata\" : { \"generated_at\" : \"2025-10-31T10:00:00\" , \"total_models\" : 3 , \"total_collections\" : 5 , \"collection_prefix\" : null }, \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"partition_00001\" , \"partition_00002\" ], \"total_documents\" : 450000 , \"partitions\" : [ \"partition_00001\" , \"partition_00002\" ] }, \"Field\" : { \"collections\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ], \"total_documents\" : 680000 , \"partitions\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ] } }, \"collection_to_models\" : { \"partition_00001\" : [ \"Table\" ], \"partition_00002\" : [ \"Table\" , \"Field\" ], \"partition_00003\" : [ \"Field\" ], \"partition_00005\" : [ \"Field\" ] } } Step 2: Initialize Query Client \u00b6 from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import os import asyncio async def main (): # Initialize client with query config async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , # or \"http\" for self-hosted cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : # Client is ready to query pass asyncio . run ( main ()) Step 3: Query Your Index \u00b6 Query All Models \u00b6 Search across all collections when you don't know which model contains the answer: async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : results = await client . query ( query_texts = [ \"What are SAP authorization objects?\" ], n_results = 10 , models = None , # Query ALL collections ) # Process results for doc_id , distance , metadata in zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): model = metadata . get ( \"model_name\" , \"unknown\" ) print ( f \"[ { model } ] { doc_id } - Distance: { distance : .4f } \" ) Query Specific Models \u00b6 Search only relevant collections when you know the model: results = await client . query ( query_texts = [ \"transaction table MARA\" ], n_results = 10 , models = [ \"Table\" ], # Only query Table collections ) Query with Metadata Filters \u00b6 Combine model filtering with ChromaDB metadata filters: results = await client . query ( query_texts = [ \"customer data\" ], n_results = 10 , where = { \"has_sem\" : True }, # Only semantic-enabled records models = [ \"Table\" , \"Field\" ], ) Complete Example \u00b6 from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import os import asyncio async def search_sap_index (): \"\"\"Complete example: query multi-collection index.\"\"\" async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : # Example 1: Broad search across all models print ( \"=== Searching all models ===\" ) results = await client . query ( query_texts = [ \"SAP authorization\" ], n_results = 5 , models = None , ) for i , ( doc_id , distance , metadata ) in enumerate ( zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ), 1 ): model = metadata . get ( \"model_name\" , \"unknown\" ) print ( f \" { i } . [ { model } ] { doc_id } - Distance: { distance : .4f } \" ) # Example 2: Targeted search in Table model print ( \" \\n === Searching Table model only ===\" ) results = await client . query ( query_texts = [ \"transaction tables\" ], n_results = 5 , models = [ \"Table\" ], ) # Example 3: Count total documents total = await client . count ( models = None ) print ( f \" \\n Total documents in index: { total : , } \" ) if __name__ == \"__main__\" : asyncio . run ( search_sap_index ()) Connection Types \u00b6 ChromaDB Cloud \u00b6 async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : pass Self-Hosted ChromaDB \u00b6 async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"http\" , http_host = \"localhost:8000\" , ) as client : pass Next Steps \u00b6 Explore API Reference for all available methods Check Examples for advanced query patterns Review Configuration for query config options Read Best Practices for performance optimization","title":"Getting Started"},{"location":"query/getting-started/#getting-started-with-query","text":"This guide shows you how to query multi-collection ChromaDB indexes created with idxr.","title":"Getting Started with Query"},{"location":"query/getting-started/#prerequisites","text":"Indexed data : You must have already indexed data using idxr vectorize index Python environment : Python 3.9+ with idxr installed ChromaDB access : Connection details for ChromaDB (local or cloud)","title":"Prerequisites"},{"location":"query/getting-started/#installation","text":"The query client is included with the idxr package: pip install idxr","title":"Installation"},{"location":"query/getting-started/#basic-workflow","text":"","title":"Basic Workflow"},{"location":"query/getting-started/#step-1-generate-query-config","text":"After indexing completes, generate a query configuration: idxr vectorize generate-query-config \\ --partition-out-dir build/vector \\ --output query_config.json \\ --model path/to/model_registry.yaml This scans all *_resume_state.json files in your partition directory and creates a mapping of model names to collection names. Output ( query_config.json ): { \"metadata\" : { \"generated_at\" : \"2025-10-31T10:00:00\" , \"total_models\" : 3 , \"total_collections\" : 5 , \"collection_prefix\" : null }, \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"partition_00001\" , \"partition_00002\" ], \"total_documents\" : 450000 , \"partitions\" : [ \"partition_00001\" , \"partition_00002\" ] }, \"Field\" : { \"collections\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ], \"total_documents\" : 680000 , \"partitions\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ] } }, \"collection_to_models\" : { \"partition_00001\" : [ \"Table\" ], \"partition_00002\" : [ \"Table\" , \"Field\" ], \"partition_00003\" : [ \"Field\" ], \"partition_00005\" : [ \"Field\" ] } }","title":"Step 1: Generate Query Config"},{"location":"query/getting-started/#step-2-initialize-query-client","text":"from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import os import asyncio async def main (): # Initialize client with query config async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , # or \"http\" for self-hosted cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : # Client is ready to query pass asyncio . run ( main ())","title":"Step 2: Initialize Query Client"},{"location":"query/getting-started/#step-3-query-your-index","text":"","title":"Step 3: Query Your Index"},{"location":"query/getting-started/#query-all-models","text":"Search across all collections when you don't know which model contains the answer: async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : results = await client . query ( query_texts = [ \"What are SAP authorization objects?\" ], n_results = 10 , models = None , # Query ALL collections ) # Process results for doc_id , distance , metadata in zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ): model = metadata . get ( \"model_name\" , \"unknown\" ) print ( f \"[ { model } ] { doc_id } - Distance: { distance : .4f } \" )","title":"Query All Models"},{"location":"query/getting-started/#query-specific-models","text":"Search only relevant collections when you know the model: results = await client . query ( query_texts = [ \"transaction table MARA\" ], n_results = 10 , models = [ \"Table\" ], # Only query Table collections )","title":"Query Specific Models"},{"location":"query/getting-started/#query-with-metadata-filters","text":"Combine model filtering with ChromaDB metadata filters: results = await client . query ( query_texts = [ \"customer data\" ], n_results = 10 , where = { \"has_sem\" : True }, # Only semantic-enabled records models = [ \"Table\" , \"Field\" ], )","title":"Query with Metadata Filters"},{"location":"query/getting-started/#complete-example","text":"from indexer.vectorize_lib import AsyncMultiCollectionQueryClient from pathlib import Path import os import asyncio async def search_sap_index (): \"\"\"Complete example: query multi-collection index.\"\"\" async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : # Example 1: Broad search across all models print ( \"=== Searching all models ===\" ) results = await client . query ( query_texts = [ \"SAP authorization\" ], n_results = 5 , models = None , ) for i , ( doc_id , distance , metadata ) in enumerate ( zip ( results [ \"ids\" ][ 0 ], results [ \"distances\" ][ 0 ], results [ \"metadatas\" ][ 0 ] ), 1 ): model = metadata . get ( \"model_name\" , \"unknown\" ) print ( f \" { i } . [ { model } ] { doc_id } - Distance: { distance : .4f } \" ) # Example 2: Targeted search in Table model print ( \" \\n === Searching Table model only ===\" ) results = await client . query ( query_texts = [ \"transaction tables\" ], n_results = 5 , models = [ \"Table\" ], ) # Example 3: Count total documents total = await client . count ( models = None ) print ( f \" \\n Total documents in index: { total : , } \" ) if __name__ == \"__main__\" : asyncio . run ( search_sap_index ())","title":"Complete Example"},{"location":"query/getting-started/#connection-types","text":"","title":"Connection Types"},{"location":"query/getting-started/#chromadb-cloud","text":"async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"cloud\" , cloud_api_key = os . getenv ( \"CHROMA_API_TOKEN\" ), ) as client : pass","title":"ChromaDB Cloud"},{"location":"query/getting-started/#self-hosted-chromadb","text":"async with AsyncMultiCollectionQueryClient ( config_path = Path ( \"query_config.json\" ), client_type = \"http\" , http_host = \"localhost:8000\" , ) as client : pass","title":"Self-Hosted ChromaDB"},{"location":"query/getting-started/#next-steps","text":"Explore API Reference for all available methods Check Examples for advanced query patterns Review Configuration for query config options Read Best Practices for performance optimization","title":"Next Steps"},{"location":"query/overview/","text":"Query \u00b6 The idxr query client provides async query capabilities for multi-collection ChromaDB indexes. When indexing large datasets (16M+ records), idxr distributes data across multiple ChromaDB collections using the PartitionCollectionStrategy . The query client enables efficient querying across these partitions with automatic fan-out, result merging, and model-based filtering. Responsibilities \u00b6 Query config generation \u2013 scan indexed partitions to build model-to-collection mappings from resume state files. Multi-collection fan-out \u2013 automatically distribute queries across relevant collections based on model filters. Parallel execution \u2013 leverage asyncio to query collections concurrently for optimal performance. Result merging \u2013 combine and rank results by distance across all queried collections. Graceful degradation \u2013 handle partial collection failures while returning available results. Workflow Summary \u00b6 Generate query config \u2013 after indexing completes, run idxr vectorize generate-query-config to create a query configuration mapping models to collections. Initialize query client \u2013 use AsyncMultiCollectionQueryClient with the generated config to connect to your ChromaDB instance. Query with model filters \u2013 specify which models to query (or models=None for all collections) and let the client handle fan-out and merging. Retrieve results \u2013 get merged, ranked results from multiple collections as if querying a single collection. Key Features \u00b6 Model-Based Filtering \u00b6 Query specific models or all models: # Query only Table and Field models results = await client . query ( query_texts = [ \"SAP transaction tables\" ], models = [ \"Table\" , \"Field\" ], ) # Query all models results = await client . query ( query_texts = [ \"authorization objects\" ], models = None , ) Automatic Collection Mapping \u00b6 The query config automatically maps model names to their collections: { \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"partition_00001\" , \"partition_00003\" ], \"total_documents\" : 450000 }, \"Field\" : { \"collections\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ], \"total_documents\" : 680000 } } } Parallel Query Execution \u00b6 Queries are executed in parallel using asyncio.gather() : Performance : Latency bound by slowest collection, not sum of all queries Concurrency : Collection-level parallelization safe for use with thread pools Resilience : Partial failures don't block successful collections Result Merging \u00b6 Results from multiple collections are: Collected from all queried collections Sorted by distance score (ascending) Limited to requested n_results Returned in standard ChromaDB query format Architecture \u00b6 User Query \u2193 AsyncMultiCollectionQueryClient \u2193 Query Config (model \u2192 collections mapping) \u2193 Fan-out to Collections (asyncio.gather) \u2193 \u251c\u2500\u2192 partition_00001.query() \u251c\u2500\u2192 partition_00002.query() \u2514\u2500\u2192 partition_00003.query() \u2193 Merge Results (by distance) \u2193 Return Top N Results Use Cases \u00b6 Broad Search (models=None) \u00b6 Use when you don't know which model contains the answer: # User asks: \"What is SAP authorization?\" # Could be in Table, Function, AuthObject, or other models results = await client . query ( query_texts = [ \"SAP authorization\" ], models = None , # Search all models ) Performance : Queries all collections (e.g., 10 partitions in parallel) Targeted Search (specific models) \u00b6 Use when you know the relevant model(s): # User asks: \"Show me transaction table MARA\" # Definitely in Table model results = await client . query ( query_texts = [ \"transaction table MARA\" ], models = [ \"Table\" ], # Only search Table collections ) Performance : Queries only relevant collections (e.g., 2 partitions in parallel) Performance Considerations \u00b6 Query Type Collections Queried Approx. Latency Best For models=None All (10 partitions) ~500ms Exploratory search, unknown model models=[\"Table\"] Subset (2 partitions) ~200ms Targeted search, known model models=[\"Table\", \"Field\"] Union (5 partitions) ~350ms Multi-model search Note : Latency is bound by the slowest collection due to parallel execution. Next Steps \u00b6 Read Getting Started for installation and basic usage Explore API Reference for detailed client documentation Check Examples for common query patterns Review Configuration for query config generation options","title":"Purpose & Workflow"},{"location":"query/overview/#query","text":"The idxr query client provides async query capabilities for multi-collection ChromaDB indexes. When indexing large datasets (16M+ records), idxr distributes data across multiple ChromaDB collections using the PartitionCollectionStrategy . The query client enables efficient querying across these partitions with automatic fan-out, result merging, and model-based filtering.","title":"Query"},{"location":"query/overview/#responsibilities","text":"Query config generation \u2013 scan indexed partitions to build model-to-collection mappings from resume state files. Multi-collection fan-out \u2013 automatically distribute queries across relevant collections based on model filters. Parallel execution \u2013 leverage asyncio to query collections concurrently for optimal performance. Result merging \u2013 combine and rank results by distance across all queried collections. Graceful degradation \u2013 handle partial collection failures while returning available results.","title":"Responsibilities"},{"location":"query/overview/#workflow-summary","text":"Generate query config \u2013 after indexing completes, run idxr vectorize generate-query-config to create a query configuration mapping models to collections. Initialize query client \u2013 use AsyncMultiCollectionQueryClient with the generated config to connect to your ChromaDB instance. Query with model filters \u2013 specify which models to query (or models=None for all collections) and let the client handle fan-out and merging. Retrieve results \u2013 get merged, ranked results from multiple collections as if querying a single collection.","title":"Workflow Summary"},{"location":"query/overview/#key-features","text":"","title":"Key Features"},{"location":"query/overview/#model-based-filtering","text":"Query specific models or all models: # Query only Table and Field models results = await client . query ( query_texts = [ \"SAP transaction tables\" ], models = [ \"Table\" , \"Field\" ], ) # Query all models results = await client . query ( query_texts = [ \"authorization objects\" ], models = None , )","title":"Model-Based Filtering"},{"location":"query/overview/#automatic-collection-mapping","text":"The query config automatically maps model names to their collections: { \"model_to_collections\" : { \"Table\" : { \"collections\" : [ \"partition_00001\" , \"partition_00003\" ], \"total_documents\" : 450000 }, \"Field\" : { \"collections\" : [ \"partition_00002\" , \"partition_00003\" , \"partition_00005\" ], \"total_documents\" : 680000 } } }","title":"Automatic Collection Mapping"},{"location":"query/overview/#parallel-query-execution","text":"Queries are executed in parallel using asyncio.gather() : Performance : Latency bound by slowest collection, not sum of all queries Concurrency : Collection-level parallelization safe for use with thread pools Resilience : Partial failures don't block successful collections","title":"Parallel Query Execution"},{"location":"query/overview/#result-merging","text":"Results from multiple collections are: Collected from all queried collections Sorted by distance score (ascending) Limited to requested n_results Returned in standard ChromaDB query format","title":"Result Merging"},{"location":"query/overview/#architecture","text":"User Query \u2193 AsyncMultiCollectionQueryClient \u2193 Query Config (model \u2192 collections mapping) \u2193 Fan-out to Collections (asyncio.gather) \u2193 \u251c\u2500\u2192 partition_00001.query() \u251c\u2500\u2192 partition_00002.query() \u2514\u2500\u2192 partition_00003.query() \u2193 Merge Results (by distance) \u2193 Return Top N Results","title":"Architecture"},{"location":"query/overview/#use-cases","text":"","title":"Use Cases"},{"location":"query/overview/#broad-search-modelsnone","text":"Use when you don't know which model contains the answer: # User asks: \"What is SAP authorization?\" # Could be in Table, Function, AuthObject, or other models results = await client . query ( query_texts = [ \"SAP authorization\" ], models = None , # Search all models ) Performance : Queries all collections (e.g., 10 partitions in parallel)","title":"Broad Search (models=None)"},{"location":"query/overview/#targeted-search-specific-models","text":"Use when you know the relevant model(s): # User asks: \"Show me transaction table MARA\" # Definitely in Table model results = await client . query ( query_texts = [ \"transaction table MARA\" ], models = [ \"Table\" ], # Only search Table collections ) Performance : Queries only relevant collections (e.g., 2 partitions in parallel)","title":"Targeted Search (specific models)"},{"location":"query/overview/#performance-considerations","text":"Query Type Collections Queried Approx. Latency Best For models=None All (10 partitions) ~500ms Exploratory search, unknown model models=[\"Table\"] Subset (2 partitions) ~200ms Targeted search, known model models=[\"Table\", \"Field\"] Union (5 partitions) ~350ms Multi-model search Note : Latency is bound by the slowest collection due to parallel execution.","title":"Performance Considerations"},{"location":"query/overview/#next-steps","text":"Read Getting Started for installation and basic usage Explore API Reference for detailed client documentation Check Examples for common query patterns Review Configuration for query config generation options","title":"Next Steps"},{"location":"vectorize/config/","text":"Vectorize Config Reference \u00b6 When you index directly from raw CSVs instead of a partition manifest, idxr vectorize reads a JSON config that mirrors the prepare-datasets layout with a couple of vectorization-specific knobs. { \"Contract\" : { \"path\" : \"datasets/contracts.csv\" , \"columns\" : { \"id\" : \"CONTRACT_ID\" , \"title\" : \"CONTRACT_TITLE\" , \"summary\" : \"DESCRIPTION\" }, \"truncation_strategy\" : \"middle_out\" } } Auto-generated : If you run idxr prepare_datasets new-config , the companion vectorize config is scaffolded automatically. Use that stub as the starting point whenever possible. Field Required Description path \u2705 Source file path. Leave blank to skip a model temporarily. columns \u2705 Mapping of model field names to CSV headers. Should align with the prepare-datasets config. truncation_strategy optional Override the truncation behaviour for this model ( end , start , middle_out , sentences ). Leave null for the global default. For partition-based pipelines you do not need this config\u2014just point idxr vectorize index at the manifest emitted by idxr prepare_datasets .","title":"Config Reference"},{"location":"vectorize/config/#vectorize-config-reference","text":"When you index directly from raw CSVs instead of a partition manifest, idxr vectorize reads a JSON config that mirrors the prepare-datasets layout with a couple of vectorization-specific knobs. { \"Contract\" : { \"path\" : \"datasets/contracts.csv\" , \"columns\" : { \"id\" : \"CONTRACT_ID\" , \"title\" : \"CONTRACT_TITLE\" , \"summary\" : \"DESCRIPTION\" }, \"truncation_strategy\" : \"middle_out\" } } Auto-generated : If you run idxr prepare_datasets new-config , the companion vectorize config is scaffolded automatically. Use that stub as the starting point whenever possible. Field Required Description path \u2705 Source file path. Leave blank to skip a model temporarily. columns \u2705 Mapping of model field names to CSV headers. Should align with the prepare-datasets config. truncation_strategy optional Override the truncation behaviour for this model ( end , start , middle_out , sentences ). Leave null for the global default. For partition-based pipelines you do not need this config\u2014just point idxr vectorize index at the manifest emitted by idxr prepare_datasets .","title":"Vectorize Config Reference"},{"location":"vectorize/overview/","text":"Vectorize \u00b6 idxr vectorize ingests partitions and upserts documents into Chroma. It is designed for long-running, resume-friendly indexing jobs that can target local persistent stores or Chroma Cloud tenants. Responsibilities \u00b6 Model validation \u2013 the same registry used by prepare_datasets drives schema-aware indexing and metadata enrichment. Token budgeting \u2013 dynamic truncation strategies ensure each document respects the embedding model\u2019s token limits. Resume state \u2013 batch offsets, row digests, and manifest progress are checkpointed so reruns skip already indexed slices. Observability \u2013 structured logs, optional log rotation, and error YAML payloads make failures debuggable. Multi-tenant support \u2013 built-in clients for local persistent stores and Chroma Cloud, with pluggable collection strategies. Workflow Summary \u00b6 Generate configs \u2013 either edit a vectorize JSON config manually or let idxr prepare_datasets generate partition manifests. Run idxr vectorize index \u2013 point at the manifest or config, choose an output location for per-partition persistence, and supply the required connectivity flags. Resume as needed \u2013 re-run with --resume to pick up where you left off after a failure or partial run. Inspect progress \u2013 use idxr vectorize status to compare manifest entries against indexed partitions and ensure the run completed. The following pages document configuration schemas and command-line flags in detail.","title":"Purpose & Workflow"},{"location":"vectorize/overview/#vectorize","text":"idxr vectorize ingests partitions and upserts documents into Chroma. It is designed for long-running, resume-friendly indexing jobs that can target local persistent stores or Chroma Cloud tenants.","title":"Vectorize"},{"location":"vectorize/overview/#responsibilities","text":"Model validation \u2013 the same registry used by prepare_datasets drives schema-aware indexing and metadata enrichment. Token budgeting \u2013 dynamic truncation strategies ensure each document respects the embedding model\u2019s token limits. Resume state \u2013 batch offsets, row digests, and manifest progress are checkpointed so reruns skip already indexed slices. Observability \u2013 structured logs, optional log rotation, and error YAML payloads make failures debuggable. Multi-tenant support \u2013 built-in clients for local persistent stores and Chroma Cloud, with pluggable collection strategies.","title":"Responsibilities"},{"location":"vectorize/overview/#workflow-summary","text":"Generate configs \u2013 either edit a vectorize JSON config manually or let idxr prepare_datasets generate partition manifests. Run idxr vectorize index \u2013 point at the manifest or config, choose an output location for per-partition persistence, and supply the required connectivity flags. Resume as needed \u2013 re-run with --resume to pick up where you left off after a failure or partial run. Inspect progress \u2013 use idxr vectorize status to compare manifest entries against indexed partitions and ensure the run completed. The following pages document configuration schemas and command-line flags in detail.","title":"Workflow Summary"},{"location":"vectorize/args/batch-size/","text":"--batch-size \u00b6 Why we added this flag: embedding APIs have rate and token limits; controlling batch size lets you trade throughput for stability. What it does \u00b6 Sets the maximum number of documents per embedding request (default: 128 ). Works alongside --token-limit to prevent oversized batches. Smaller batches reduce retry storms when your model registry contains very large documents. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --batch-size 300 \\ --truncation-strategy middle_out Tips \u00b6 Increase cautiously\u2014monitor OpenAI rate limits and Chroma ingest timing. Pair with --parallel-partitions to achieve higher concurrency without breaching per-request limits.","title":"--batch-size"},{"location":"vectorize/args/batch-size/#-batch-size","text":"Why we added this flag: embedding APIs have rate and token limits; controlling batch size lets you trade throughput for stability.","title":"--batch-size"},{"location":"vectorize/args/batch-size/#what-it-does","text":"Sets the maximum number of documents per embedding request (default: 128 ). Works alongside --token-limit to prevent oversized batches. Smaller batches reduce retry storms when your model registry contains very large documents.","title":"What it does"},{"location":"vectorize/args/batch-size/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --batch-size 300 \\ --truncation-strategy middle_out","title":"Typical usage"},{"location":"vectorize/args/batch-size/#tips","text":"Increase cautiously\u2014monitor OpenAI rate limits and Chroma ingest timing. Pair with --parallel-partitions to achieve higher concurrency without breaching per-request limits.","title":"Tips"},{"location":"vectorize/args/chroma-api-token/","text":"--chroma-api-token \u00b6 Why we added this flag: both the HTTP and Cloud Chroma clients require authentication; surfacing the API token keeps credentials out of code. What it does \u00b6 Supplies the bearer token used for HTTP/Cloud authentication. Required whenever --client-type=http or --client-type=cloud is used (unless the server allows anonymous access). Ignored for local persistent runs. Typical usage \u00b6 idxr vectorize index \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions Tips \u00b6 Prefer export CHROMA_API_TOKEN=... and pass --chroma-api-token \"$CHROMA_API_TOKEN\" to avoid leaking secrets in shell history. Tokens expire; automate rotation and run idxr vectorize status after rotations to ensure new jobs still authenticate.","title":"--chroma-api-token"},{"location":"vectorize/args/chroma-api-token/#-chroma-api-token","text":"Why we added this flag: both the HTTP and Cloud Chroma clients require authentication; surfacing the API token keeps credentials out of code.","title":"--chroma-api-token"},{"location":"vectorize/args/chroma-api-token/#what-it-does","text":"Supplies the bearer token used for HTTP/Cloud authentication. Required whenever --client-type=http or --client-type=cloud is used (unless the server allows anonymous access). Ignored for local persistent runs.","title":"What it does"},{"location":"vectorize/args/chroma-api-token/#typical-usage","text":"idxr vectorize index \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions","title":"Typical usage"},{"location":"vectorize/args/chroma-api-token/#tips","text":"Prefer export CHROMA_API_TOKEN=... and pass --chroma-api-token \"$CHROMA_API_TOKEN\" to avoid leaking secrets in shell history. Tokens expire; automate rotation and run idxr vectorize status after rotations to ensure new jobs still authenticate.","title":"Tips"},{"location":"vectorize/args/chroma-cloud-database/","text":"--chroma-cloud-database \u00b6 Why we added this flag: Chroma Cloud segregates data into databases within a tenant; the database header ensures idxr writes to the intended logical database. What it does \u00b6 Specifies the database identifier when --client-type=cloud . Sets the X-Chroma-Database header on every request. Required for Chroma Cloud runs; ignored elsewhere. Typical usage \u00b6 idxr vectorize index \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions Tips \u00b6 Name databases by environment (e.g., staging , production ) to simplify credential management. --chroma-database is an alias that behaves identically.","title":"--chroma-cloud-database"},{"location":"vectorize/args/chroma-cloud-database/#-chroma-cloud-database","text":"Why we added this flag: Chroma Cloud segregates data into databases within a tenant; the database header ensures idxr writes to the intended logical database.","title":"--chroma-cloud-database"},{"location":"vectorize/args/chroma-cloud-database/#what-it-does","text":"Specifies the database identifier when --client-type=cloud . Sets the X-Chroma-Database header on every request. Required for Chroma Cloud runs; ignored elsewhere.","title":"What it does"},{"location":"vectorize/args/chroma-cloud-database/#typical-usage","text":"idxr vectorize index \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions","title":"Typical usage"},{"location":"vectorize/args/chroma-cloud-database/#tips","text":"Name databases by environment (e.g., staging , production ) to simplify credential management. --chroma-database is an alias that behaves identically.","title":"Tips"},{"location":"vectorize/args/chroma-cloud-tenant/","text":"--chroma-cloud-tenant \u00b6 Why we added this flag: Chroma Cloud isolates data per tenant; the tenant header must accompany every API call, so idxr surfaces it explicitly. What it does \u00b6 Specifies the tenant identifier when --client-type=cloud . Sets the X-Chroma-Tenant header on every request. Required for Chroma Cloud runs; ignored for persistent or HTTP clients. Typical usage \u00b6 idxr vectorize index \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions Tips \u00b6 Environment variables keep sensitive values out of shell history: export CHROMA_TENANT=... and pass --chroma-cloud-tenant \"$CHROMA_TENANT\" . --chroma-tenant is an alias; use either name for compatibility with older scripts.","title":"--chroma-cloud-tenant"},{"location":"vectorize/args/chroma-cloud-tenant/#-chroma-cloud-tenant","text":"Why we added this flag: Chroma Cloud isolates data per tenant; the tenant header must accompany every API call, so idxr surfaces it explicitly.","title":"--chroma-cloud-tenant"},{"location":"vectorize/args/chroma-cloud-tenant/#what-it-does","text":"Specifies the tenant identifier when --client-type=cloud . Sets the X-Chroma-Tenant header on every request. Required for Chroma Cloud runs; ignored for persistent or HTTP clients.","title":"What it does"},{"location":"vectorize/args/chroma-cloud-tenant/#typical-usage","text":"idxr vectorize index \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions","title":"Typical usage"},{"location":"vectorize/args/chroma-cloud-tenant/#tips","text":"Environment variables keep sensitive values out of shell history: export CHROMA_TENANT=... and pass --chroma-cloud-tenant \"$CHROMA_TENANT\" . --chroma-tenant is an alias; use either name for compatibility with older scripts.","title":"Tips"},{"location":"vectorize/args/client-type/","text":"--client-type \u00b6 Why we added this flag: idxr needs to talk to multiple Chroma deployments\u2014local persistent stores, self-hosted HTTP servers, and Chroma Cloud\u2014so we expose an explicit selector. What it does \u00b6 Chooses the Chroma client implementation: persistent (default) \u2013 local duckdb-backed store at --persist-dir . http \u2013 self-hosted HTTP server. cloud \u2013 Chroma Cloud tenant. Determines which additional connectivity flags are required. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX Tips \u00b6 Stick with persistent for local development; switch to cloud when deploying to managed Chroma. The HTTP and Cloud clients share most of the same flags; the cloud alias preconfigures headers for you.","title":"--client-type"},{"location":"vectorize/args/client-type/#-client-type","text":"Why we added this flag: idxr needs to talk to multiple Chroma deployments\u2014local persistent stores, self-hosted HTTP servers, and Chroma Cloud\u2014so we expose an explicit selector.","title":"--client-type"},{"location":"vectorize/args/client-type/#what-it-does","text":"Chooses the Chroma client implementation: persistent (default) \u2013 local duckdb-backed store at --persist-dir . http \u2013 self-hosted HTTP server. cloud \u2013 Chroma Cloud tenant. Determines which additional connectivity flags are required.","title":"What it does"},{"location":"vectorize/args/client-type/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --client-type cloud \\ --chroma-cloud-tenant tenant_id \\ --chroma-cloud-database db_name \\ --chroma-api-token ck-XXXXXXXX","title":"Typical usage"},{"location":"vectorize/args/client-type/#tips","text":"Stick with persistent for local development; switch to cloud when deploying to managed Chroma. The HTTP and Cloud clients share most of the same flags; the cloud alias preconfigures headers for you.","title":"Tips"},{"location":"vectorize/args/collection/","text":"--collection \u00b6 Why we added this flag: Chroma stores documents in named collections; idxr needs to know which collection to upsert into (or prefix when partitioning). What it does \u00b6 Identifies the target collection. Required for persistent client runs; optional for partition-aware Cloud runs where each partition uses its own collection name derived from metadata. Used when generating resume state filenames ( <collection>_resume_state.json ). Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --client-type cloud \\ --collection ecc-std Tips \u00b6 Namespace collections by environment (e.g., kb-prod , kb-staging ) to simplify cross-env testing. When using partition-specific collections, include the base collection name in observability dashboards so metrics aggregate correctly.","title":"--collection"},{"location":"vectorize/args/collection/#-collection","text":"Why we added this flag: Chroma stores documents in named collections; idxr needs to know which collection to upsert into (or prefix when partitioning).","title":"--collection"},{"location":"vectorize/args/collection/#what-it-does","text":"Identifies the target collection. Required for persistent client runs; optional for partition-aware Cloud runs where each partition uses its own collection name derived from metadata. Used when generating resume state filenames ( <collection>_resume_state.json ).","title":"What it does"},{"location":"vectorize/args/collection/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --client-type cloud \\ --collection ecc-std","title":"Typical usage"},{"location":"vectorize/args/collection/#tips","text":"Namespace collections by environment (e.g., kb-prod , kb-staging ) to simplify cross-env testing. When using partition-specific collections, include the base collection name in observability dashboards so metrics aggregate correctly.","title":"Tips"},{"location":"vectorize/args/config/","text":"--config \u00b6 Why we added this flag: when you index straight from CSVs (without using prepare-datasets), you still need a declarative mapping\u2014this flag points vectorize to that JSON config. What it does \u00b6 Path to the vectorize config file (see Config Reference ). Mutually exclusive with --partition-manifest . Choose one source of truth. Required for direct CSV indexing; optional otherwise. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --config configs/vectorize_contracts.json \\ --collection ecc-std \\ --persist-dir workdir/chroma Tips \u00b6 Prefer manifest-driven indexing in production; use config-driven runs for quick experiments or backfills. Keep vectorize and prepare-datasets configs in sync so column renames stay consistent.","title":"--config"},{"location":"vectorize/args/config/#-config","text":"Why we added this flag: when you index straight from CSVs (without using prepare-datasets), you still need a declarative mapping\u2014this flag points vectorize to that JSON config.","title":"--config"},{"location":"vectorize/args/config/#what-it-does","text":"Path to the vectorize config file (see Config Reference ). Mutually exclusive with --partition-manifest . Choose one source of truth. Required for direct CSV indexing; optional otherwise.","title":"What it does"},{"location":"vectorize/args/config/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --config configs/vectorize_contracts.json \\ --collection ecc-std \\ --persist-dir workdir/chroma","title":"Typical usage"},{"location":"vectorize/args/config/#tips","text":"Prefer manifest-driven indexing in production; use config-driven runs for quick experiments or backfills. Keep vectorize and prepare-datasets configs in sync so column renames stay consistent.","title":"Tips"},{"location":"vectorize/args/log-file/","text":"--log-file \u00b6 Why we added this flag: long-running indexing jobs deserve persistent logs with rotation; this flag activates file logging alongside (or instead of) console output. What it does \u00b6 When provided, idxr writes logs to the specified path using rotating file handlers. Supports dynamic filenames (e.g., timestamped) to separate individual runs. Pair with --log-max-bytes , --log-backup-count , and --log-no-console for full control. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --log-file workdir/logs/vectorize.log \\ --log-max-bytes 209715200 \\ --log-level INFO Tips \u00b6 Use absolute paths when running under systemd or cron so logs land where you expect. Combine with log rotation (below) to prevent disk bloat on permanent ingestion workers.","title":"--log-file"},{"location":"vectorize/args/log-file/#-log-file","text":"Why we added this flag: long-running indexing jobs deserve persistent logs with rotation; this flag activates file logging alongside (or instead of) console output.","title":"--log-file"},{"location":"vectorize/args/log-file/#what-it-does","text":"When provided, idxr writes logs to the specified path using rotating file handlers. Supports dynamic filenames (e.g., timestamped) to separate individual runs. Pair with --log-max-bytes , --log-backup-count , and --log-no-console for full control.","title":"What it does"},{"location":"vectorize/args/log-file/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --log-file workdir/logs/vectorize.log \\ --log-max-bytes 209715200 \\ --log-level INFO","title":"Typical usage"},{"location":"vectorize/args/log-file/#tips","text":"Use absolute paths when running under systemd or cron so logs land where you expect. Combine with log rotation (below) to prevent disk bloat on permanent ingestion workers.","title":"Tips"},{"location":"vectorize/args/log-level/","text":"--log-level \u00b6 Why we added this flag: vectorization spans long network calls; adjustable verbosity helps distinguish steady-state noise from actionable errors. What it does \u00b6 Sets the logging verbosity for all subcommands ( DEBUG , INFO , WARNING , ERROR ). Defaults to INFO . Works with both console and file handlers. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --log-level INFO Tips \u00b6 Switch to DEBUG during incident response; revert to INFO for production to limit log volume. Combine with --log-no-console when running in environments where STDOUT is noisy or rate-limited.","title":"--log-level"},{"location":"vectorize/args/log-level/#-log-level","text":"Why we added this flag: vectorization spans long network calls; adjustable verbosity helps distinguish steady-state noise from actionable errors.","title":"--log-level"},{"location":"vectorize/args/log-level/#what-it-does","text":"Sets the logging verbosity for all subcommands ( DEBUG , INFO , WARNING , ERROR ). Defaults to INFO . Works with both console and file handlers.","title":"What it does"},{"location":"vectorize/args/log-level/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --log-level INFO","title":"Typical usage"},{"location":"vectorize/args/log-level/#tips","text":"Switch to DEBUG during incident response; revert to INFO for production to limit log volume. Combine with --log-no-console when running in environments where STDOUT is noisy or rate-limited.","title":"Tips"},{"location":"vectorize/args/log-max-bytes/","text":"--log-max-bytes \u00b6 Why we added this flag: when file logging is enabled, we need a cap on file size so long-running jobs do not fill disks. What it does \u00b6 Defines the rotation threshold (in bytes) for the log file handler. Defaults to 104857600 (100\u202fMB). Only relevant when --log-file is set. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --log-file workdir/logs/vectorize.log \\ --log-max-bytes 209715200 Tips \u00b6 Pair with --log-backup-count to define total retained log size. Tune based on infrastructure\u2014smaller caps rotate more frequently but may split stack traces across files.","title":"--log-max-bytes"},{"location":"vectorize/args/log-max-bytes/#-log-max-bytes","text":"Why we added this flag: when file logging is enabled, we need a cap on file size so long-running jobs do not fill disks.","title":"--log-max-bytes"},{"location":"vectorize/args/log-max-bytes/#what-it-does","text":"Defines the rotation threshold (in bytes) for the log file handler. Defaults to 104857600 (100\u202fMB). Only relevant when --log-file is set.","title":"What it does"},{"location":"vectorize/args/log-max-bytes/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --log-file workdir/logs/vectorize.log \\ --log-max-bytes 209715200","title":"Typical usage"},{"location":"vectorize/args/log-max-bytes/#tips","text":"Pair with --log-backup-count to define total retained log size. Tune based on infrastructure\u2014smaller caps rotate more frequently but may split stack traces across files.","title":"Tips"},{"location":"vectorize/args/model/","text":"--model \u00b6 Why we added this flag: vectorization must use the same model registry as prepare-datasets to ensure metadata alignment and schema validation. What it does \u00b6 Accepts a Python import path ( package.module:ATTRIBUTE ) resolving to a Mapping[str, ModelSpec] . Drives per-model token policies, metadata enrichment, and schema signature validation during indexing. Required for every idxr vectorize command. Typical usage \u00b6 idxr vectorize index \\ --model kb.std.ecc_6_0_ehp_7.registry:MODEL_REGISTRY \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions Tips \u00b6 Keep a shared environment variable ( IDXR_MODEL ) so all automation references the same registry string. If you maintain multiple registries, note them in your manifest to avoid mixing incompatible partitions.","title":"--model"},{"location":"vectorize/args/model/#-model","text":"Why we added this flag: vectorization must use the same model registry as prepare-datasets to ensure metadata alignment and schema validation.","title":"--model"},{"location":"vectorize/args/model/#what-it-does","text":"Accepts a Python import path ( package.module:ATTRIBUTE ) resolving to a Mapping[str, ModelSpec] . Drives per-model token policies, metadata enrichment, and schema signature validation during indexing. Required for every idxr vectorize command.","title":"What it does"},{"location":"vectorize/args/model/#typical-usage","text":"idxr vectorize index \\ --model kb.std.ecc_6_0_ehp_7.registry:MODEL_REGISTRY \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions","title":"Typical usage"},{"location":"vectorize/args/model/#tips","text":"Keep a shared environment variable ( IDXR_MODEL ) so all automation references the same registry string. If you maintain multiple registries, note them in your manifest to avoid mixing incompatible partitions.","title":"Tips"},{"location":"vectorize/args/parallel-partitions/","text":"--parallel-partitions \u00b6 Why we added this flag: when manifests contain many partitions, running them sequentially can take hours; limited parallelism keeps throughput high while respecting API quotas. What it does \u00b6 Controls how many partitions idxr indexes concurrently (default: 1 ). Only applies to manifest-driven runs. Each worker maintains its own resume state to ensure idempotency. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --parallel-partitions 4 Tips \u00b6 Match the value to your embedding API rate limits. Start with 2 and scale up while monitoring throttling. Ensure the machine hosting idxr has enough CPU and memory\u2014each partition spawns its own worker thread.","title":"--parallel-partitions"},{"location":"vectorize/args/parallel-partitions/#-parallel-partitions","text":"Why we added this flag: when manifests contain many partitions, running them sequentially can take hours; limited parallelism keeps throughput high while respecting API quotas.","title":"--parallel-partitions"},{"location":"vectorize/args/parallel-partitions/#what-it-does","text":"Controls how many partitions idxr indexes concurrently (default: 1 ). Only applies to manifest-driven runs. Each worker maintains its own resume state to ensure idempotency.","title":"What it does"},{"location":"vectorize/args/parallel-partitions/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --parallel-partitions 4","title":"Typical usage"},{"location":"vectorize/args/parallel-partitions/#tips","text":"Match the value to your embedding API rate limits. Start with 2 and scale up while monitoring throttling. Ensure the machine hosting idxr has enough CPU and memory\u2014each partition spawns its own worker thread.","title":"Tips"},{"location":"vectorize/args/partition-dir/","text":"--partition-dir \u00b6 Why we added this flag: the status command needs to read partition metadata even when the manifest is unavailable; this flag lets you point directly at the partition directory tree. What it does \u00b6 Path to the folder containing partition subdirectories (typically the same --output-root used during preparation). Used by idxr vectorize status to compare manifest entries against indexed outputs. Optional when the manifest is accessible; required otherwise. Typical usage \u00b6 idxr vectorize status \\ --model \" $IDXR_MODEL \" \\ --partition-dir workdir/partitions \\ --partition-out-dir workdir/chroma_partitions Tips \u00b6 Pair with --partition-out-dir to get a concise summary of which partitions have completed indexing. Include the status command in CI to fail fast when an indexing job leaves partitions partially processed.","title":"--partition-dir"},{"location":"vectorize/args/partition-dir/#-partition-dir","text":"Why we added this flag: the status command needs to read partition metadata even when the manifest is unavailable; this flag lets you point directly at the partition directory tree.","title":"--partition-dir"},{"location":"vectorize/args/partition-dir/#what-it-does","text":"Path to the folder containing partition subdirectories (typically the same --output-root used during preparation). Used by idxr vectorize status to compare manifest entries against indexed outputs. Optional when the manifest is accessible; required otherwise.","title":"What it does"},{"location":"vectorize/args/partition-dir/#typical-usage","text":"idxr vectorize status \\ --model \" $IDXR_MODEL \" \\ --partition-dir workdir/partitions \\ --partition-out-dir workdir/chroma_partitions","title":"Typical usage"},{"location":"vectorize/args/partition-dir/#tips","text":"Pair with --partition-out-dir to get a concise summary of which partitions have completed indexing. Include the status command in CI to fail fast when an indexing job leaves partitions partially processed.","title":"Tips"},{"location":"vectorize/args/partition-manifest/","text":"--partition-manifest \u00b6 Why we added this flag: partition-aware indexing needs to replay manifest \u201cmigrations\u201d in order; this flag points vectorize at the manifest produced by idxr prepare_datasets . What it does \u00b6 Path to the manifest.json emitted by prepare-datasets. Determines which partitions to index, in which order, and with which schema version metadata. Mutually exclusive with --config ; choose manifest-driven or direct CSV configs. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --collection ecc-std Tips \u00b6 Keep the manifest under version control for auditability. If you want to replay a subset, copy the manifest, edit the partitions list, and point the command at the trimmed copy.","title":"--partition-manifest"},{"location":"vectorize/args/partition-manifest/#-partition-manifest","text":"Why we added this flag: partition-aware indexing needs to replay manifest \u201cmigrations\u201d in order; this flag points vectorize at the manifest produced by idxr prepare_datasets .","title":"--partition-manifest"},{"location":"vectorize/args/partition-manifest/#what-it-does","text":"Path to the manifest.json emitted by prepare-datasets. Determines which partitions to index, in which order, and with which schema version metadata. Mutually exclusive with --config ; choose manifest-driven or direct CSV configs.","title":"What it does"},{"location":"vectorize/args/partition-manifest/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --collection ecc-std","title":"Typical usage"},{"location":"vectorize/args/partition-manifest/#tips","text":"Keep the manifest under version control for auditability. If you want to replay a subset, copy the manifest, edit the partitions list, and point the command at the trimmed copy.","title":"Tips"},{"location":"vectorize/args/partition-out-dir/","text":"--partition-out-dir \u00b6 Why we added this flag: each partition needs a durable home for embedding state, logs, and error payloads; this flag declares the root directory for index artifacts. What it does \u00b6 Directory where per-partition resume files, error YAMLs, and compacted documents are stored. Required for manifest-driven indexing. Can be used with --persist-dir for local Chroma or implicitly with cloud clients. Reusing the same directory allows --resume to pick up unfinished partitions. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --resume Tips \u00b6 Keep this directory on a fast, reliable volume; idxr writes incremental checkpoints frequently. Rotate old partitions by archiving subdirectories once the manifest marks them as complete or dropped.","title":"--partition-out-dir"},{"location":"vectorize/args/partition-out-dir/#-partition-out-dir","text":"Why we added this flag: each partition needs a durable home for embedding state, logs, and error payloads; this flag declares the root directory for index artifacts.","title":"--partition-out-dir"},{"location":"vectorize/args/partition-out-dir/#what-it-does","text":"Directory where per-partition resume files, error YAMLs, and compacted documents are stored. Required for manifest-driven indexing. Can be used with --persist-dir for local Chroma or implicitly with cloud clients. Reusing the same directory allows --resume to pick up unfinished partitions.","title":"What it does"},{"location":"vectorize/args/partition-out-dir/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --resume","title":"Typical usage"},{"location":"vectorize/args/partition-out-dir/#tips","text":"Keep this directory on a fast, reliable volume; idxr writes incremental checkpoints frequently. Rotate old partitions by archiving subdirectories once the manifest marks them as complete or dropped.","title":"Tips"},{"location":"vectorize/args/resume/","text":"--resume \u00b6 Why we added this flag: large indexing jobs rarely finish in a single attempt; resume mode skips already processed rows so restarts are cheap. What it does \u00b6 Tells idxr to read the resume state file and skip models/partitions that have not changed. Works for both config-driven and manifest-driven runs. Automatically updates the resume state file after each successful model/partition. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --resume Tips \u00b6 Resume state is stored next to --persist-dir / --partition-out-dir . Back it up before big upgrades. Combine with --batch-size tuning to recover faster after transient API failures.","title":"--resume"},{"location":"vectorize/args/resume/#-resume","text":"Why we added this flag: large indexing jobs rarely finish in a single attempt; resume mode skips already processed rows so restarts are cheap.","title":"--resume"},{"location":"vectorize/args/resume/#what-it-does","text":"Tells idxr to read the resume state file and skip models/partitions that have not changed. Works for both config-driven and manifest-driven runs. Automatically updates the resume state file after each successful model/partition.","title":"What it does"},{"location":"vectorize/args/resume/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --resume","title":"Typical usage"},{"location":"vectorize/args/resume/#tips","text":"Resume state is stored next to --persist-dir / --partition-out-dir . Back it up before big upgrades. Combine with --batch-size tuning to recover faster after transient API failures.","title":"Tips"},{"location":"vectorize/args/status-partition-dir/","text":"status --partition-dir \u00b6 Why we added this flag: sometimes the manifest is unavailable, but the partition directory tree still exists; status can scan it directly when you provide this path. What it does \u00b6 Points idxr vectorize status at the directory containing partition subfolders and vectorize_config.json files. Optional but recommended; it reveals partitions that were configured yet never indexed. Complements --partition-out-dir , which inspects the output side of the pipeline. Typical usage \u00b6 idxr vectorize status \\ --model \" $IDXR_MODEL \" \\ --partition-dir workdir/partitions \\ --partition-out-dir workdir/chroma_partitions Tips \u00b6 Provide both directories to cross-check configured vs. completed partitions. Handy for post-mortem scripts validating that every expected model reached the index.","title":"status --partition-dir"},{"location":"vectorize/args/status-partition-dir/#status-partition-dir","text":"Why we added this flag: sometimes the manifest is unavailable, but the partition directory tree still exists; status can scan it directly when you provide this path.","title":"status --partition-dir"},{"location":"vectorize/args/status-partition-dir/#what-it-does","text":"Points idxr vectorize status at the directory containing partition subfolders and vectorize_config.json files. Optional but recommended; it reveals partitions that were configured yet never indexed. Complements --partition-out-dir , which inspects the output side of the pipeline.","title":"What it does"},{"location":"vectorize/args/status-partition-dir/#typical-usage","text":"idxr vectorize status \\ --model \" $IDXR_MODEL \" \\ --partition-dir workdir/partitions \\ --partition-out-dir workdir/chroma_partitions","title":"Typical usage"},{"location":"vectorize/args/status-partition-dir/#tips","text":"Provide both directories to cross-check configured vs. completed partitions. Handy for post-mortem scripts validating that every expected model reached the index.","title":"Tips"},{"location":"vectorize/args/status-partition-out-dir/","text":"status --partition-out-dir \u00b6 Why we added this flag: the status subcommand inspects actual index outputs; it needs to know where idxr vectorize index wrote its per-partition artifacts. What it does \u00b6 Points idxr vectorize status at the root directory that contains partition-specific resume files and chunk metadata. Required for the status command. Used to determine which partitions are complete, in progress, or missing. Typical usage \u00b6 idxr vectorize status \\ --model \" $IDXR_MODEL \" \\ --partition-out-dir workdir/chroma_partitions Tips \u00b6 Run status immediately after indexing; it highlights partitions that never started or were interrupted mid-run. Store this directory on persistent storage so status remains accurate after restarts.","title":"status --partition-out-dir"},{"location":"vectorize/args/status-partition-out-dir/#status-partition-out-dir","text":"Why we added this flag: the status subcommand inspects actual index outputs; it needs to know where idxr vectorize index wrote its per-partition artifacts.","title":"status --partition-out-dir"},{"location":"vectorize/args/status-partition-out-dir/#what-it-does","text":"Points idxr vectorize status at the root directory that contains partition-specific resume files and chunk metadata. Required for the status command. Used to determine which partitions are complete, in progress, or missing.","title":"What it does"},{"location":"vectorize/args/status-partition-out-dir/#typical-usage","text":"idxr vectorize status \\ --model \" $IDXR_MODEL \" \\ --partition-out-dir workdir/chroma_partitions","title":"Typical usage"},{"location":"vectorize/args/status-partition-out-dir/#tips","text":"Run status immediately after indexing; it highlights partitions that never started or were interrupted mid-run. Store this directory on persistent storage so status remains accurate after restarts.","title":"Tips"},{"location":"vectorize/args/truncation-strategy/","text":"--truncation-strategy \u00b6 Why we added this flag: different document shapes need different trimming strategies when they exceed token limits; this switch lets you pick the global default. What it does \u00b6 Sets the default truncation behaviour before embedding. Choices: end , start , middle_out , sentences , auto (default). Per-model overrides are possible via the vectorize config. Typical usage \u00b6 idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --truncation-strategy middle_out Tips \u00b6 Leave auto enabled until you understand your content mix; idxr chooses strategies based on schema hints. Switch to middle_out for highly structured tables so you keep both headers and footers. Monitor the logs\u2014idxr reports truncation decisions along with token savings.","title":"--truncation-strategy"},{"location":"vectorize/args/truncation-strategy/#-truncation-strategy","text":"Why we added this flag: different document shapes need different trimming strategies when they exceed token limits; this switch lets you pick the global default.","title":"--truncation-strategy"},{"location":"vectorize/args/truncation-strategy/#what-it-does","text":"Sets the default truncation behaviour before embedding. Choices: end , start , middle_out , sentences , auto (default). Per-model overrides are possible via the vectorize config.","title":"What it does"},{"location":"vectorize/args/truncation-strategy/#typical-usage","text":"idxr vectorize index \\ --model \" $IDXR_MODEL \" \\ --partition-manifest workdir/partitions/manifest.json \\ --partition-out-dir workdir/chroma_partitions \\ --truncation-strategy middle_out","title":"Typical usage"},{"location":"vectorize/args/truncation-strategy/#tips","text":"Leave auto enabled until you understand your content mix; idxr chooses strategies based on schema hints. Switch to middle_out for highly structured tables so you keep both headers and footers. Monitor the logs\u2014idxr reports truncation decisions along with token savings.","title":"Tips"}]}